{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1 – Geschichte und Agenten\n",
        "\n",
        "Dieses Notebook veranschaulicht das Agentenmodell und setzt es in Beziehung zur KI-Geschichte. Es ist als Live-Demo gedacht und nutzt entweder **aima-python** oder eine minimale Eigenimplementierung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup aima-python (optional)\n",
        "\n",
        "Wenn du aima-python lokal oder auf Kaggle nutzen willst, kannst du das Repo installieren. Falls Installation nicht möglich ist, überspringe diesen Block und nutze die Minimal-Demo darunter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "## Optional: aima-python installieren\n",
        "# !pip -q install git+https://github.com/aimacode/aima-python.git\n",
        "# Hinweis: In Kaggle ggf. Internet aktivieren. Lokal ggf. in venv.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent und Umgebung als Minimalmodell\n",
        "\n",
        "Ein einfacher Reflex-Agent reagiert nur auf die aktuelle Wahrnehmung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class SimpleEnv:\n",
        "    size: int = 5\n",
        "    goal: int = 4\n",
        "\n",
        "    def reset(self):\n",
        "        self.pos = 0\n",
        "        return self.pos\n",
        "\n",
        "    def step(self, action):\n",
        "        # action: -1 links, +1 rechts\n",
        "        self.pos = max(0, min(self.size-1, self.pos + action))\n",
        "        reward = 1 if self.pos == self.goal else -0.01\n",
        "        done = self.pos == self.goal\n",
        "        return self.pos, reward, done\n",
        "\n",
        "def reflex_agent(obs):\n",
        "    # Immer nach rechts\n",
        "    return +1\n",
        "\n",
        "env = SimpleEnv()\n",
        "obs = env.reset()\n",
        "total = 0\n",
        "for t in range(20):\n",
        "    action = reflex_agent(obs)\n",
        "    obs, r, done = env.step(action)\n",
        "    total += r\n",
        "    if done:\n",
        "        break\n",
        "total, obs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Historischer Kontrast als Kurz-Timeline\n",
        "\n",
        "Ergänze hier deine eigene Timeline aus Session 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "timeline = ['1956 Dartmouth', '1970s Expertensysteme', 'KI-Winter', '1990s Lernen aus Daten', '2010s Deep Learning']\n",
        "for t in timeline:\n",
        "    print('-', t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-Übung\n",
        "\n",
        "Ändere die Agentenregel so, dass der Agent manchmal zufällig handelt. Vergleiche die Gesamtbelohnung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def noisy_reflex(obs, p=0.2):\n",
        "    return random.choice([-1, +1]) if random.random() < p else +1\n",
        "\n",
        "scores=[]\n",
        "for _ in range(50):\n",
        "    obs=env.reset(); total=0\n",
        "    for t in range(30):\n",
        "        a=noisy_reflex(obs, p=0.2)\n",
        "        obs,r,done=env.step(a)\n",
        "        total+=r\n",
        "        if done: break\n",
        "    scores.append(total)\n",
        "\n",
        "pd.Series(scores).describe()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "title": "Notebook 1 – Geschichte und Agenten"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}