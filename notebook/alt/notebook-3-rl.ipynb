{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"title":"Notebook 3 – Reinforcement Learning (didaktisch)","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"df63a4c3-32a2-43d8-94b9-1b752f323606","cell_type":"markdown","source":"# Reinforcement Learning in der MiniGridworld\n\nDieses Notebook ist auf **Verständlichkeit** und **Interaktivität** ausgelegt.  \nEs führt in kleinen Schritten von einer Minimalwelt zu einer 2D-MiniGridworld und zeigt:\n\n- Agent, Umgebung, Reward, Episode\n- Exploration vs. Exploitation\n- Q-Learning als Lernregel\n- Interaktives Optimieren von Parametern (Hyperparameter)\n- Vergleich: Greedy-Heuristik vs. Q-Learning\n- Challenge World (40×20): Greedy scheitert, Q-Learning kann besser werden\n\n> Hinweis: Für die MiniGridworld brauchst du die Klasse `MiniGridworld`.  \n> Falls sie nicht vorhanden ist, nutze die kompakte Version in der nächsten Zelle.\n","metadata":{}},{"id":"2878649d-1f76-43f5-b39e-fef917f90c8b","cell_type":"markdown","source":"## 0. MiniGridworld (kompakte Kernklasse)\nFalls du `MiniGridworld` schon aus Notebook 1 hast, kannst du diese Zelle überspringen.\n","metadata":{}},{"id":"103b0251-7f57-4eee-b176-5d2deeeff21b","cell_type":"code","source":"# MiniGridworld (kompakte Kernklasse)\n# API: reset() -> state, step(action) -> next_state, reward, done, info\n# render(trail=...) -> ASCII, state_id(state) -> int\n\nclass MiniGridworld:\n    ACTIONS = {\n        0: (0, -1),  # up\n        1: (1, 0),   # right\n        2: (0, 1),   # down\n        3: (-1, 0),  # left\n    }\n\n    def __init__(self, width=8, height=6, start=(0,0), goal=(7,5),\n                 step_penalty=-0.02, goal_reward=1.0, max_steps=200,\n                 walls=None, seed=0):\n        self.width = width\n        self.height = height\n        self.start = start\n        self.goal = goal\n        self.step_penalty = step_penalty\n        self.goal_reward = goal_reward\n        self.max_steps = max_steps\n        self.walls = set(walls) if walls else set()\n        self.seed = seed\n        self.reset()\n\n    def _in_bounds(self, p):\n        x, y = p\n        return 0 <= x < self.width and 0 <= y < self.height\n\n    def reset(self):\n        self.pos = self.start\n        self.t = 0\n        return self.pos\n\n    def step(self, action):\n        self.t += 1\n        dx, dy = self.ACTIONS[int(action)]\n        nxt = (self.pos[0] + dx, self.pos[1] + dy)\n\n        # ungültig oder Wand -> stehen bleiben\n        if (not self._in_bounds(nxt)) or (nxt in self.walls):\n            nxt = self.pos\n\n        self.pos = nxt\n\n        done = False\n        reward = self.step_penalty\n\n        if self.pos == self.goal:\n            reward = self.goal_reward\n            done = True\n\n        if self.t >= self.max_steps:\n            done = True\n\n        return self.pos, reward, done, {\"t\": self.t}\n\n    def render(self, trail=None):\n        trail = trail or set()\n        lines = []\n        for y in range(self.height):\n            row = []\n            for x in range(self.width):\n                p = (x, y)\n                c = \".\"\n                if p in self.walls: c = \"#\"\n                if p in trail: c = \"*\"\n                if p == self.goal: c = \"G\"\n                if p == self.pos: c = \"M\"\n                row.append(c)\n            lines.append(\" \".join(row))\n        return \"\\n\".join(lines)\n\n    def state_id(self, state=None):\n        if state is None:\n            state = self.pos\n        x, y = state\n        return y * self.width + x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:19:44.156408Z","iopub.execute_input":"2026-02-12T21:19:44.156792Z","iopub.status.idle":"2026-02-12T21:19:44.175329Z","shell.execute_reply.started":"2026-02-12T21:19:44.156760Z","shell.execute_reply":"2026-02-12T21:19:44.174104Z"}},"outputs":[],"execution_count":1},{"id":"96966dc1-dd2c-472d-87de-bf17b4c9a483","cell_type":"markdown","source":"## 1. Idee in Pseudocode (Q-Learning)\n\n**Begriffe**\n- Zustand `s`: aktuelle Situation des Agenten\n- Aktion `a`: Entscheidung\n- Reward `r`: Belohnung oder Strafe nach der Aktion\n- Q-Wert `Q(s,a)`: Qualitätsschätzung für Aktion `a` in Zustand `s`\n\n**Update-Regel**\n`Q(s,a) ← Q(s,a) + α · ( r + γ · max_a' Q(s',a') − Q(s,a) )`\n\n- `α` Lernrate\n- `γ` Zukunftsgewicht\n- `ε` Exploration (Zufallsschritte)\n","metadata":{}},{"id":"76d7f2c3-ffcb-45e2-b8bc-6d9baa6db8f9","cell_type":"markdown","source":"## 2. Imports und Hilfsfunktionen\n","metadata":{}},{"id":"f8954c70-0e42-4ccd-bbe1-01f1bd2e3c8b","cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:20:00.092058Z","iopub.execute_input":"2026-02-12T21:20:00.092395Z","iopub.status.idle":"2026-02-12T21:20:02.058347Z","shell.execute_reply.started":"2026-02-12T21:20:00.092367Z","shell.execute_reply":"2026-02-12T21:20:02.056863Z"}},"outputs":[],"execution_count":2},{"id":"4a458b84-cfc3-47e1-ba90-dbf544256062","cell_type":"markdown","source":"## 3. MiniGridworld erzeugen (Größe, Hindernisse, Rewards)\n\n- `width`, `height`: Weltgröße  \n- `density`: Hindernisdichte  \n- `seed`: reproduzierbares Layout  \n- `step_penalty`: Schrittstrafe  \n- `goal_reward`: Zielbelohnung  \n","metadata":{}},{"id":"22c22620-53ca-4619-8ea4-f7e2fb7c35e0","cell_type":"code","source":"def generate_random_walls(width, height, start, goal, density=0.18, seed=0):\n    rng = random.Random(seed)\n    walls=set()\n    for y in range(height):\n        for x in range(width):\n            p=(x,y)\n            if p == start or p == goal:\n                continue\n            if rng.random() < density:\n                walls.add(p)\n    return walls\n\ndef make_env(width=12, height=8, density=0.18, seed=0,\n             step_penalty=-0.02, goal_reward=1.0, max_steps=300):\n    start=(0,0)\n    goal=(width-1, height-1)\n    walls = generate_random_walls(width, height, start, goal, density=density, seed=seed)\n    env = MiniGridworld(width=width, height=height, start=start, goal=goal,\n                        step_penalty=step_penalty, goal_reward=goal_reward,\n                        max_steps=max_steps, walls=walls, seed=seed)\n    return env\n\n# Sichttest\nenv = make_env(width=12, height=8, density=0.20, seed=3)\nprint(env.render())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:20:05.208098Z","iopub.execute_input":"2026-02-12T21:20:05.209451Z","iopub.status.idle":"2026-02-12T21:20:05.220800Z","shell.execute_reply.started":"2026-02-12T21:20:05.209407Z","shell.execute_reply":"2026-02-12T21:20:05.219684Z"}},"outputs":[{"name":"stdout","text":"M . . . . . # # . . . .\n. . . . # . . . . . # .\n. . # . . . . . . . . .\n. . # # . . . . . . . .\n. . . . . . . . # . . .\n. . . . . . # . . # . .\n# . . . # . # . . . . .\n. . # . # # . . # # . G\n","output_type":"stream"}],"execution_count":3},{"id":"c401a62a-ae8e-4e01-b710-994030de446f","cell_type":"markdown","source":"## Markov-Eigenschaft und MDP (warum Q-Learning funktioniert)\n\nEine Umgebung ist **Markov**, wenn für die Zukunft nur der aktuelle Zustand zählt:\n\nP(s_{t+1} | s_t, a_t) ist unabhängig von der ganzen Vergangenheit.\n\nDann lässt sich das Problem als **Markov-Entscheidungsprozess (MDP)** beschreiben:\n- Zustände S, Aktionen A\n- Übergänge P(s'|s,a)\n- Rewards R(s,a,s')\n- Diskontfaktor γ (Weitblick)\n\nWichtig: Wenn der Zustand *nicht* alle relevanten Informationen enthält, ist das Problem **nicht-Markov**,\nund man braucht „Gedächtnis“ oder einen erweiterten Zustand (State Augmentation).","metadata":{}},{"id":"6bdc330a-7a7f-4814-9253-279767819189","cell_type":"code","source":"# Markov-Demo: Warum \"Zustand\" manchmal erweitert werden muss\n#\n# Idee: Eine Regel hängt davon ab, ob ein Schalter schon einmal betreten wurde.\n# Wenn der Zustand nur (x,y) ist, fehlt Information -> nicht-Markov.\n# Lösung: Zustand erweitern um \"switch_on\" (Gedächtnis als Teil des Zustands).\n\nclass SwitchWorld:\n    # Aktionen wie MiniGridworld: 0 up, 1 right, 2 down, 3 left\n    ACTIONS = {0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)}\n\n    def __init__(self, width=6, height=4, start=(0,0), goal=(5,3), switch=(2,1)):\n        self.width, self.height = width, height\n        self.start, self.goal, self.switch = start, goal, switch\n        self.reset()\n\n    def reset(self):\n        self.pos = self.start\n        self.switch_on = False\n        return (self.pos, self.switch_on)\n\n    def step(self, action):\n        dx, dy = self.ACTIONS[int(action)]\n        x, y = self.pos\n        nx, ny = max(0, min(self.width-1, x+dx)), max(0, min(self.height-1, y+dy))\n        self.pos = (nx, ny)\n\n        # \"Gedächtnis\": wenn Switch besucht wurde, bleibt er an\n        if self.pos == self.switch:\n            self.switch_on = True\n\n        # Reward-Logik hängt vom Gedächtnis ab\n        done = (self.pos == self.goal)\n        if done:\n            reward = 1.0 if self.switch_on else -1.0  # ohne Switch ist Ziel \"schlecht\"\n        else:\n            reward = -0.01\n\n        return (self.pos, self.switch_on), reward, done\n\nenv = SwitchWorld()\n\n# Zwei Trajektorien: beide enden im gleichen (x,y)=goal, aber mit unterschiedlichem switch_on\ndef run_path(actions):\n    obs = env.reset()\n    total=0\n    for a in actions:\n        obs, r, done = env.step(a)\n        total += r\n        if done:\n            break\n    return obs, total\n\n# Path A: direkt zum Ziel (ohne Switch)\nactions_direct = [1,1,1,1,1, 2,2,2]   # rechts, dann runter\n\n# Path B: Umweg über Switch, dann Ziel\nactions_switch = [1,1, 2, 1, 1,1, 2,2]  # erst zum switch, dann weiter\n\nobsA, retA = run_path(actions_direct)\nobsB, retB = run_path(actions_switch)\n\nprint(\"Ende A:\", obsA, \"Return:\", round(retA,3))\nprint(\"Ende B:\", obsB, \"Return:\", round(retB,3))\n\nprint(\"\\nInterpretation:\")\nprint(\"- Gleiche Position am Ende, aber unterschiedlicher Reward, weil Vergangenheit (Switch) zählt.\")\nprint(\"- Damit das Markov wird, muss 'switch_on' Teil des Zustands sein (Zustand erweitern).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:45:06.058498Z","iopub.execute_input":"2026-02-12T21:45:06.059839Z","iopub.status.idle":"2026-02-12T21:45:06.074885Z","shell.execute_reply.started":"2026-02-12T21:45:06.059725Z","shell.execute_reply":"2026-02-12T21:45:06.073733Z"}},"outputs":[{"name":"stdout","text":"Ende A: ((5, 3), False) Return: -1.07\nEnde B: ((5, 3), True) Return: 0.93\n\nInterpretation:\n- Gleiche Position am Ende, aber unterschiedlicher Reward, weil Vergangenheit (Switch) zählt.\n- Damit das Markov wird, muss 'switch_on' Teil des Zustands sein (Zustand erweitern).\n","output_type":"stream"}],"execution_count":20},{"id":"e3eaeca3-3042-44e5-9c1d-06c53ab63e82","cell_type":"markdown","source":"## 4. Verlustfunktion (Loss) für Parametervergleich\n\nWir definieren eine Zielgröße, die wir **minimieren** wollen:\n\n`Loss = w1 · (1 − SuccessRate) + w2 · (MeanSteps / MaxSteps) + w3 · Var(Return)`\n\n- niedrig ist gut  \n- misst: Ziel erreichen, schnell sein, stabil sein  \n","metadata":{}},{"id":"7088f222-0b20-4cdb-89f3-86707c6aadde","cell_type":"code","source":"def loss_function(success_rate, mean_steps, max_steps, returns, w1=1.0, w2=0.3, w3=0.1):\n    return (w1 * (1.0 - success_rate)\n            + w2 * (mean_steps / max_steps)\n            + w3 * float(np.var(returns)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:49:56.403960Z","iopub.execute_input":"2026-02-12T21:49:56.405019Z","iopub.status.idle":"2026-02-12T21:49:56.411337Z","shell.execute_reply.started":"2026-02-12T21:49:56.404983Z","shell.execute_reply":"2026-02-12T21:49:56.410161Z"}},"outputs":[],"execution_count":21},{"id":"a4ca5162-ab44-4352-951e-717f7e7f0c51","cell_type":"markdown","source":"## 5. Greedy-Baseline (Heuristik)\n\nGreedy ist **kein Lernen**: pro Zustand wird die Aktion gewählt, die die Manhattan-Distanz zum Ziel reduziert.\nDas ist eine hilfreiche Baseline, kann aber in Sackgassen stecken bleiben.\n","metadata":{}},{"id":"a796037b-9d06-4757-852e-9eb6b53203f7","cell_type":"code","source":"ACTIONS = [0,1,2,3]  # up,right,down,left\n\ndef manhattan(s, g):\n    return abs(s[0]-g[0]) + abs(s[1]-g[1])\n\ndef greedy_policy_from_env(env):\n    n_states = env.width * env.height\n    pol = np.zeros(n_states, dtype=int)\n\n    for y in range(env.height):\n        for x in range(env.width):\n            s = (x, y)\n            if s in env.walls:\n                pol[env.state_id(s)] = 0\n                continue\n\n            best_a, best_d = 0, 10**9\n            for a in ACTIONS:\n                # peek ohne den Zustand dauerhaft zu verändern\n                old_pos, old_t = env.pos, env.t\n                env.pos, env.t = s, 0\n                sp, r, done, info = env.step(a)\n                env.pos, env.t = old_pos, old_t\n\n                d = manhattan(sp, env.goal)\n                if d < best_d:\n                    best_d = d\n                    best_a = a\n\n            pol[env.state_id(s)] = best_a\n\n    return pol\n\ndef evaluate_with_returns(env, policy, episodes=50):\n    rets=[]; steps=[]; succ=0\n    for _ in range(episodes):\n        s = env.reset()\n        sid = env.state_id(s)\n        total = 0.0\n        for t in range(env.max_steps):\n            a = int(policy[sid])\n            s, r, done, info = env.step(a)\n            total += r\n            sid = env.state_id(s)\n            if done:\n                succ += int(s == env.goal)\n                steps.append(info.get(\"t\", t+1))\n                break\n        else:\n            steps.append(env.max_steps)\n        rets.append(total)\n    return np.array(rets), np.array(steps), succ/episodes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:50:00.082709Z","iopub.execute_input":"2026-02-12T21:50:00.084000Z","iopub.status.idle":"2026-02-12T21:50:00.096015Z","shell.execute_reply.started":"2026-02-12T21:50:00.083964Z","shell.execute_reply":"2026-02-12T21:50:00.094659Z"}},"outputs":[],"execution_count":22},{"id":"5c318bff-adcc-4a29-8f2f-a723458720f9","cell_type":"markdown","source":"## 6. Q-Learning (Training) + Policy\n\nWir lernen eine Q-Tabelle und leiten daraus eine Policy ab (beste Aktion pro Zustand).\n","metadata":{}},{"id":"592c3ea2-4602-4304-8b2d-19a8bae16c18","cell_type":"code","source":"def policy_from_Q(Q):\n    return np.argmax(Q, axis=1)\n\ndef q_learning_train(env, episodes=800, alpha=0.5, gamma=0.9, eps=0.2):\n    n_states = env.width * env.height\n    Q = np.zeros((n_states, 4), dtype=float)\n\n    def eps_greedy(sid):\n        if random.random() < eps:\n            return random.choice(ACTIONS)\n        return int(np.argmax(Q[sid]))\n\n    returns=[]\n    for ep in range(episodes):\n        s = env.reset()\n        sid = env.state_id(s)\n        total = 0.0\n\n        for t in range(env.max_steps):\n            a = eps_greedy(sid)\n            sp, r, done, info = env.step(a)\n            spid = env.state_id(sp)\n\n            td_target = r + gamma * np.max(Q[spid])\n            Q[sid, a] += alpha * (td_target - Q[sid, a])\n\n            total += r\n            sid = spid\n            if done:\n                break\n\n        returns.append(total)\n\n    return Q, returns\n\ndef q_learning_train_decay(env, episodes=900, alpha=0.5, gamma=0.95, eps_start=0.35, eps_end=0.05):\n    n_states = env.width * env.height\n    Q = np.zeros((n_states, 4), dtype=float)\n\n    for ep in range(episodes):\n        eps = eps_start + (eps_end - eps_start) * (ep / max(1, episodes-1))\n\n        s = env.reset()\n        sid = env.state_id(s)\n\n        for t in range(env.max_steps):\n            if np.random.rand() < eps:\n                a = np.random.randint(0, 4)\n            else:\n                a = int(np.argmax(Q[sid]))\n\n            sp, r, done, info = env.step(a)\n            spid = env.state_id(sp)\n\n            td_target = r + gamma * np.max(Q[spid])\n            Q[sid, a] += alpha * (td_target - Q[sid, a])\n\n            sid = spid\n            if done:\n                break\n\n    return Q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:50:11.271493Z","iopub.execute_input":"2026-02-12T21:50:11.272242Z","iopub.status.idle":"2026-02-12T21:50:11.284326Z","shell.execute_reply.started":"2026-02-12T21:50:11.272191Z","shell.execute_reply":"2026-02-12T21:50:11.283049Z"}},"outputs":[],"execution_count":23},{"id":"ce42fc31-dd51-4892-b1f3-f66f73b9e664","cell_type":"markdown","source":"## 7. Interaktiv: Q-Learning trainieren und danach als ASCII-Lauf ansehen\n","metadata":{}},{"id":"7978f6a2-e36b-4f6a-b54a-478d516d02ac","cell_type":"code","source":"try:\n    import ipywidgets as w\n    from IPython.display import display, clear_output\nexcept Exception as e:\n    print(\"ipywidgets nicht verfügbar:\", e)\n    raise\n\nbtn = w.Button(description=\"Train Q-Learning\", button_style=\"primary\")\n\nW = w.IntSlider(value=20, min=10, max=40, step=1, description=\"width\")\nH = w.IntSlider(value=10, min=5, max=20, step=1, description=\"height\")\nD = w.FloatSlider(value=0.20, min=0.00, max=0.40, step=0.02, description=\"density\")\nS = w.IntSlider(value=3, min=0, max=99, step=1, description=\"seed\")\n\nSP = w.FloatSlider(value=-0.02, min=-0.30, max=0.00, step=0.01, description=\"penalty\")\nGR = w.FloatSlider(value=1.0, min=0.2, max=2.0, step=0.1, description=\"goal_reward\")\nMS = w.IntSlider(value=250, min=50, max=800, step=25, description=\"max_steps\")\n\nEP = w.IntSlider(value=800, min=50, max=4000, step=50, description=\"episodes\")\nAL = w.FloatSlider(value=0.5, min=0.05, max=1.0, step=0.05, description=\"alpha\")\nGA = w.FloatSlider(value=0.9, min=0.0, max=0.99, step=0.05, description=\"gamma\")\nEI = w.FloatSlider(value=0.2, min=0.0, max=1.0, step=0.05, description=\"eps\")\n\nEVAL = w.IntSlider(value=50, min=10, max=200, step=10, description=\"eval_eps\")\n\nout = w.Output()\n\ndisplay(w.VBox([\n    w.HBox([btn]),\n    w.HBox([W, H, D, S]),\n    w.HBox([SP, GR, MS]),\n    w.HBox([EP, AL, GA, EI, EVAL]),\n    out\n]))\n\n_last_env = None\n_last_policy = None\n_last_returns = None\n\ndef on_click(_):\n    global _last_env, _last_policy, _last_returns\n\n    env = make_env(W.value, H.value, D.value, S.value, SP.value, GR.value, MS.value)\n    Q, returns = q_learning_train(env, episodes=int(EP.value), alpha=float(AL.value), gamma=float(GA.value), eps=float(EI.value))\n    pol = policy_from_Q(Q)\n\n    env_eval = make_env(W.value, H.value, D.value, S.value, SP.value, GR.value, MS.value)\n    rets, steps, succ = evaluate_with_returns(env_eval, pol, episodes=int(EVAL.value))\n\n    with out:\n        out.clear_output(wait=True)\n        plt.figure()\n        plt.plot(returns)\n        plt.xlabel(\"Episode\")\n        plt.ylabel(\"Return (Training)\")\n        plt.show()\n\n        display(pd.DataFrame([{\n            \"success_rate_eval\": succ,\n            \"mean_steps_eval\": float(steps.mean()),\n            \"var_return_eval\": float(np.var(rets)),\n            \"env_size\": f\"{W.value}x{H.value}\",\n            \"density\": D.value,\n            \"seed\": S.value,\n            \"alpha\": AL.value, \"gamma\": GA.value, \"eps\": EI.value,\n            \"train_episodes\": int(EP.value)\n        }]))\n\n        print(\"Als Nächstes: die ASCII-Run-Zelle ausführen.\")\n\n    _last_env = env\n    _last_policy = pol\n    _last_returns = returns\n\nbtn.on_click(on_click)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:50:18.814381Z","iopub.execute_input":"2026-02-12T21:50:18.815419Z","iopub.status.idle":"2026-02-12T21:50:18.872116Z","shell.execute_reply.started":"2026-02-12T21:50:18.815371Z","shell.execute_reply":"2026-02-12T21:50:18.870969Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='primary', description='Train Q-Learning', style=ButtonStyle…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7db9a073543434aa6996fae27095141"}},"metadata":{}}],"execution_count":24},{"id":"c95e0179-7c04-41e5-95e9-09dbd470af5d","cell_type":"markdown","source":"### 7.1 ASCII-Run der gelernten Policy\n","metadata":{}},{"id":"6dce2add-a077-44ea-8125-9fa53934d00e","cell_type":"code","source":"import time\nimport ipywidgets as w\nfrom IPython.display import display\n\nbtn_run = w.Button(description=\"Run policy (ASCII)\", button_style=\"success\")\ndelay = w.FloatSlider(value=0.05, min=0.01, max=0.30, step=0.01, description=\"delay\")\ntrail_on = w.Checkbox(value=True, description=\"trail\")\nout2 = w.Output()\n\ndisplay(w.VBox([w.HBox([btn_run, delay, trail_on]), out2]))\n\ndef animate_policy(env, policy, out, delay=0.05, show_trail=True):\n    s = env.reset()\n    sid = env.state_id(s)\n    total = 0.0\n    trail=set([s])\n\n    with out:\n        out.clear_output(wait=True)\n        for t in range(env.max_steps):\n            if show_trail:\n                trail.add(s)\n\n            out.clear_output(wait=True)\n            print(f\"t={t:03d} state={s} return={total:.2f}\")\n            print(env.render(trail=trail if show_trail else None))\n\n            a = int(policy[sid])\n            s, r, done, info = env.step(a)\n            total += r\n            sid = env.state_id(s)\n\n            time.sleep(delay)\n            if done:\n                out.clear_output(wait=True)\n                print(f\"Ende: steps={info.get('t', t+1)} return={total:.2f} reached_goal={s==env.goal}\")\n                print(env.render(trail=trail if show_trail else None))\n                break\n\ndef on_run(_):\n    if _last_env is None or _last_policy is None:\n        with out2:\n            out2.clear_output(wait=True)\n            print(\"Bitte zuerst oben Q-Learning trainieren (Train Q-Learning).\")\n        return\n    animate_policy(_last_env, _last_policy, out2, delay=float(delay.value), show_trail=bool(trail_on.value))\n\nbtn_run.on_click(on_run)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:55:21.570986Z","iopub.execute_input":"2026-02-12T21:55:21.571393Z","iopub.status.idle":"2026-02-12T21:55:21.599678Z","shell.execute_reply.started":"2026-02-12T21:55:21.571364Z","shell.execute_reply":"2026-02-12T21:55:21.598571Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='success', description='Run policy (ASCII)', style=ButtonSty…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e1ba1e69f1c40a596fde74867c7a590"}},"metadata":{}}],"execution_count":31},{"id":"3cbd4b1c-c513-45e3-9e1e-2eb93599e6a3","cell_type":"markdown","source":"## 8. Vergleich: Greedy vs Q-Learning (inkl. Loss)\n\nHier vergleichen wir beide Methoden auf derselben Welt.\n","metadata":{}},{"id":"dbcdcd7c-5697-4497-b0dc-074f343cd687","cell_type":"code","source":"# Einstellungen für den Vergleich (du kannst sie ändern)\nwidth, height = 12, 8\ndensity, seed = 0.20, 3\nstep_penalty, goal_reward, max_steps = -0.02, 1.0, 250\nepisodes_eval = 60\nepisodes_train = 1000\nalpha, gamma, eps = 0.5, 0.9, 0.2\n\nw1, w2, w3 = 1.0, 0.3, 0.1\n\n# Greedy\nenv_base = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\ngreedy_pol = greedy_policy_from_env(env_base)\nenv_eval_g = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\nrets_g, steps_g, succ_g = evaluate_with_returns(env_eval_g, greedy_pol, episodes=episodes_eval)\nloss_g = loss_function(succ_g, steps_g.mean(), max_steps, rets_g, w1=w1, w2=w2, w3=w3)\n\n# Q-Learning\nenv_train = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\nQ, returns_train = q_learning_train(env_train, episodes=episodes_train, alpha=alpha, gamma=gamma, eps=eps)\nq_pol = policy_from_Q(Q)\n\nenv_eval_q = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\nrets_q, steps_q, succ_q = evaluate_with_returns(env_eval_q, q_pol, episodes=episodes_eval)\nloss_q = loss_function(succ_q, steps_q.mean(), max_steps, rets_q, w1=w1, w2=w2, w3=w3)\n\ndf_metrics = pd.DataFrame([\n    {\"method\":\"Greedy\", \"success_rate\":succ_g, \"mean_steps\":float(steps_g.mean()), \"var_return\":float(np.var(rets_g)), \"loss\":float(loss_g)},\n    {\"method\":\"Q-Learning\", \"success_rate\":succ_q, \"mean_steps\":float(steps_q.mean()), \"var_return\":float(np.var(rets_q)), \"loss\":float(loss_q)},\n]).sort_values(\"loss\")\n\ndisplay(df_metrics)\n\ndf_params = pd.DataFrame([{\"method\":\"Q-Learning\", \"train_episodes\":episodes_train, \"alpha\":alpha, \"gamma\":gamma, \"eps\":eps}])\ndisplay(df_params)\n\nplt.figure()\nplt.plot(returns_train)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Return (Training)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:55:26.482371Z","iopub.execute_input":"2026-02-12T21:55:26.483271Z","iopub.status.idle":"2026-02-12T21:55:26.851032Z","shell.execute_reply.started":"2026-02-12T21:55:26.483209Z","shell.execute_reply":"2026-02-12T21:55:26.849844Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"       method  success_rate  mean_steps    var_return    loss\n0      Greedy           1.0        18.0  1.232595e-32  0.0216\n1  Q-Learning           1.0        18.0  1.232595e-32  0.0216","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>method</th>\n      <th>success_rate</th>\n      <th>mean_steps</th>\n      <th>var_return</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Greedy</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.232595e-32</td>\n      <td>0.0216</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q-Learning</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.232595e-32</td>\n      <td>0.0216</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       method  train_episodes  alpha  gamma  eps\n0  Q-Learning            1000    0.5    0.9  0.2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>method</th>\n      <th>train_episodes</th>\n      <th>alpha</th>\n      <th>gamma</th>\n      <th>eps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q-Learning</td>\n      <td>1000</td>\n      <td>0.5</td>\n      <td>0.9</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":32},{"id":"e86e76ed-2410-480e-82ab-59521703ddd7","cell_type":"markdown","source":"## 9. Challenge World (40×20): Greedy soll scheitern\n\nWichtig: Wir trainieren **nicht** für viele Seeds.  \nStattdessen:\n1) schnelle Greedy-Vorfilterung über wenige Seeds  \n2) Q-Learning nur für 1–3 Kandidaten-Seeds\n","metadata":{}},{"id":"ec89bbd5-8225-41b8-9205-87209a9fd266","cell_type":"code","source":"# Challenge-Einstellungen (groß + schwierig, aber noch handhabbar)\nwidth, height = 40, 20\ndensity = 0.32\nstep_penalty = -0.02\ngoal_reward = 1.0\nmax_steps = 900\n\nepisodes_eval_greedy = 25\nepisodes_eval = 40\nepisodes_train = 900\n\nalpha, gamma, eps0 = 0.5, 0.95, 0.35\nw1, w2, w3 = 1.0, 0.3, 0.1\n\n# 1) Greedy Seeds vorfiltern\ngreedy_rows = []\nfor seed in range(0, 12):\n    env = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n    gpol = greedy_policy_from_env(env)\n\n    env_eval = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n    rets, steps, succ = evaluate_with_returns(env_eval, gpol, episodes=episodes_eval_greedy)\n\n    greedy_rows.append({\"seed\": seed, \"greedy_success\": succ, \"greedy_mean_steps\": float(steps.mean())})\n\ndf_g = pd.DataFrame(greedy_rows).sort_values([\"greedy_success\", \"greedy_mean_steps\"])\ndisplay(df_g)\n\ncandidate_seeds = df_g.head(3)[\"seed\"].tolist()\nprint(\"Candidate seeds:\", candidate_seeds)\n\n# 2) Für Kandidaten: Greedy vs Q-Learning (decay) vergleichen\nrows=[]\nfor seed in candidate_seeds:\n    # Greedy eval\n    env_base = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n    gpol = greedy_policy_from_env(env_base)\n    env_eval_g = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n    rets_g, steps_g, succ_g = evaluate_with_returns(env_eval_g, gpol, episodes=episodes_eval)\n    loss_g = loss_function(succ_g, steps_g.mean(), max_steps, rets_g, w1=w1, w2=w2, w3=w3)\n\n    # Q-Learning decay train + eval\n    env_train = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n    Q = q_learning_train_decay(env_train, episodes=episodes_train, alpha=alpha, gamma=gamma, eps_start=eps0, eps_end=0.05)\n    qpol = policy_from_Q(Q)\n\n    env_eval_q = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n    rets_q, steps_q, succ_q = evaluate_with_returns(env_eval_q, qpol, episodes=episodes_eval)\n    loss_q = loss_function(succ_q, steps_q.mean(), max_steps, rets_q, w1=w1, w2=w2, w3=w3)\n\n    rows.append({\n        \"seed\": seed,\n        \"greedy_success\": succ_g, \"greedy_steps\": float(steps_g.mean()), \"greedy_loss\": float(loss_g),\n        \"q_success\": succ_q, \"q_steps\": float(steps_q.mean()), \"q_loss\": float(loss_q),\n        \"episodes_train\": episodes_train\n    })\n\ndf_cmp = pd.DataFrame(rows).sort_values(\"greedy_success\")\ndisplay(df_cmp)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:55:32.769317Z","iopub.execute_input":"2026-02-12T21:55:32.769646Z","iopub.status.idle":"2026-02-12T21:55:44.616839Z","shell.execute_reply.started":"2026-02-12T21:55:32.769621Z","shell.execute_reply":"2026-02-12T21:55:44.615923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"    seed  greedy_success  greedy_mean_steps\n0      0             0.0              900.0\n1      1             0.0              900.0\n2      2             0.0              900.0\n3      3             0.0              900.0\n4      4             0.0              900.0\n5      5             0.0              900.0\n6      6             0.0              900.0\n7      7             0.0              900.0\n8      8             0.0              900.0\n9      9             0.0              900.0\n10    10             0.0              900.0\n11    11             0.0              900.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>greedy_success</th>\n      <th>greedy_mean_steps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>0.0</td>\n      <td>900.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Candidate seeds: [0, 1, 2]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   seed  greedy_success  greedy_steps  greedy_loss  q_success  q_steps  \\\n0     0             0.0         900.0          1.3        1.0     66.0   \n1     1             0.0         900.0          1.3        0.0    900.0   \n2     2             0.0         900.0          1.3        1.0     60.0   \n\n   q_loss  episodes_train  \n0   0.022             900  \n1   1.300             900  \n2   0.020             900  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>greedy_success</th>\n      <th>greedy_steps</th>\n      <th>greedy_loss</th>\n      <th>q_success</th>\n      <th>q_steps</th>\n      <th>q_loss</th>\n      <th>episodes_train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>900.0</td>\n      <td>1.3</td>\n      <td>1.0</td>\n      <td>66.0</td>\n      <td>0.022</td>\n      <td>900</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>900.0</td>\n      <td>1.3</td>\n      <td>0.0</td>\n      <td>900.0</td>\n      <td>1.300</td>\n      <td>900</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>900.0</td>\n      <td>1.3</td>\n      <td>1.0</td>\n      <td>60.0</td>\n      <td>0.020</td>\n      <td>900</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"id":"0e69378b-5b61-4da3-b56b-a31b5b6036d4","cell_type":"markdown","source":"### 9.1 Besten Seed wählen und Q-Policy für ASCII-Run vorbereiten\n","metadata":{}},{"id":"45ae7b19-3e58-45ab-ae96-0084950b1b7a","cell_type":"code","source":"if \"df_cmp\" not in globals() or df_cmp is None or len(df_cmp) == 0:\n    raise ValueError(\"df_cmp ist nicht vorhanden. Bitte zuerst die Challenge-Zelle davor ausführen.\")\n\nbest_seed = int(df_cmp.sort_values(\"q_loss\").iloc[0][\"seed\"])\nprint(\"Gewählter Seed (min q_loss):\", best_seed)\n\nenv_train = make_env(width, height, density, best_seed, step_penalty, goal_reward, max_steps)\nQ = q_learning_train_decay(env_train, episodes=episodes_train, alpha=alpha, gamma=gamma, eps_start=eps0, eps_end=0.05)\nq_pol = policy_from_Q(Q)\n\nenv_eval_q = make_env(width, height, density, best_seed, step_penalty, goal_reward, max_steps)\nrets_q, steps_q, succ_q = evaluate_with_returns(env_eval_q, q_pol, episodes=episodes_eval)\nloss_q = loss_function(succ_q, steps_q.mean(), max_steps, rets_q, w1=w1, w2=w2, w3=w3)\n\nprint(\"Q-Learning success_rate:\", succ_q, \"mean_steps:\", float(steps_q.mean()), \"loss:\", float(loss_q))\nprint(\"Jetzt kannst du die ASCII-Run-Zelle unten ausführen (für 40x20 ist das viel Output).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:55:51.364886Z","iopub.execute_input":"2026-02-12T21:55:51.365198Z","iopub.status.idle":"2026-02-12T21:55:52.988222Z","shell.execute_reply.started":"2026-02-12T21:55:51.365172Z","shell.execute_reply":"2026-02-12T21:55:52.987311Z"}},"outputs":[{"name":"stdout","text":"Gewählter Seed (min q_loss): 2\nQ-Learning success_rate: 1.0 mean_steps: 60.0 loss: 0.02\nJetzt kannst du die ASCII-Run-Zelle unten ausführen (für 40x20 ist das viel Output).\n","output_type":"stream"}],"execution_count":34},{"id":"cb6208ce-c010-4d8b-b6aa-dfee3080324f","cell_type":"markdown","source":"### 9.2 Optional: ASCII-Run in der Challenge World (viel Text)\n","metadata":{}},{"id":"e40de3fc-4ccc-4996-9c7a-f7489d8be1a0","cell_type":"code","source":"import time\n\nif \"best_seed\" not in globals() or \"q_pol\" not in globals():\n    raise ValueError(\"best_seed oder q_pol fehlt. Bitte erst die Seed-Auswahl-Zelle ausführen.\")\n\nenv_run = make_env(width, height, density, best_seed, step_penalty, goal_reward, max_steps)\n\ns = env_run.reset()\nsid = env_run.state_id(s)\ntotal = 0.0\ntrail = set([s])\n\nshow_trail = True\ndelay_sec = 0.002\n\nfor t in range(env_run.max_steps):\n    a = int(q_pol[sid])\n    s, r, done, info = env_run.step(a)\n    total += r\n    sid = env_run.state_id(s)\n    trail.add(s)\n    time.sleep(delay_sec)\n    if done:\n        break\n\nprint(f\"Ende: steps={info.get('t', t+1)} return={total:.2f} reached_goal={s==env_run.goal}\")\nprint(env_run.render(trail=trail if show_trail else None))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T21:55:54.225199Z","iopub.execute_input":"2026-02-12T21:55:54.225603Z","iopub.status.idle":"2026-02-12T21:55:54.363718Z","shell.execute_reply.started":"2026-02-12T21:55:54.225571Z","shell.execute_reply":"2026-02-12T21:55:54.362192Z"}},"outputs":[{"name":"stdout","text":"Ende: steps=60 return=-0.18 reached_goal=True\n* * . # # . . . # . . . # . . . . . . . # # # . # . . . . # # . # . . . # . . .\n. * * * . . # . . . . . . . . . . . . . . . . # . . # . # . # . . . . . . # # .\n. . # * * . # . # . # # . # . . # # . . . # . # . # . # . # # . # # . . # . # #\n. . . # * # * * * * * * * * # . # # . . # . . . # . # . . # . # # # # # . . . .\n. # . # * * * . # . . # # * # . # # # # # . . # . # . . . . . . # . . # # . . .\n. . # . . # . # # . . # . * . # . # # . . # . # . . . . . # # # . . . . # . . .\n. . . . # . . . . . . # # * # . . . . . # . . # . # # . . . # # # . . . . # . .\n. # . . # . . . . . . . # * # . . # . . . . . . . . . . . . . # . . . . . # . .\n. . . # . . . . . # . . # * * # . # . . # . . # . # # . . . . . . . # . # # . #\n. . . . . . # # . . # # . . * * * * . # . . . . # . # . . . . . # . . . . . . #\n# . # # . # . . . # # . # . # . . * * * . # # # . . . . # . . . . . . . . . . .\n. . . . . # . . . . # . . . . # . # # * * * * * * . # . . # . # # . # # # # . #\n. # # . . . . # # . . . # . . . . . . . . . . # * * * * * . . . . . # # . # . .\n. # # . # # . # # . # . . # . # . . # . # # . . . . . # * # # # # # # . . . . .\n. . # # . . # . # . . . . . . . # . . # . . . # . # # . * * * * * # . . . . # #\n# . . # . . . . # # . . # . . . . . . . # . # . # . . . . . # . * * * * . . # #\n. # . . # # . . # . . . . # # . # # . # # # # . . . . . # . # # . . # * * . . .\n. # # . . # . . . . . # # # . # . . . . . . . # # . . # . . # . . . # . * * * *\n. # # . # # . . . . . # . # . . # # . . . . . . . # . . . . . . . . # . . # . *\n. . # . . . . . # # . . . . # . # # . . . . # . # # # # . . # . # . . . . # . M\n","output_type":"stream"}],"execution_count":35},{"id":"2ceff8fd-3914-43ef-b8ad-079495cd40d9","cell_type":"markdown","source":"## 10. Mini-Übungen (ohne Code schreiben)\n\n1) Setze `eps` im interaktiven Training auf **0.0** und dann auf **0.3**.  \n   Was ändert sich an Erfolgsrate und Schrittzahl?\n\n2) Setze `gamma` auf **0.2** und auf **0.95**.  \n   Wann wirkt der Agent „kurzsichtig“, wann „weitblickend“?\n\n3) Erhöhe `density` und beobachte, ab wann Lernen deutlich schwieriger wird.\n\n4) Erhöhe die Schrittstrafe (z. B. `-0.02` → `-0.08`).  \n   Warum sinkt der Return selbst bei erfolgreichen Läufen?\n","metadata":{}}]}