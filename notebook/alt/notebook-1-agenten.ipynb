{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"title":"Notebook 1 – Geschichte und Agenten","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"647e3870-6887-493d-8b15-71391c0db369","cell_type":"markdown","source":"# Notebook 1 – Agenten\n\nDieses Notebook veranschaulicht das Agentenmodell. Es ist als Live-Demo gedacht und nutzt entweder **aima-python** oder eine minimale Eigenimplementierung.","metadata":{}},{"id":"c78d5262-7675-4ff3-8cc2-4c7b152eb2d9","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:09:49.756178Z","iopub.execute_input":"2026-02-12T19:09:49.756525Z","iopub.status.idle":"2026-02-12T19:09:49.760820Z","shell.execute_reply.started":"2026-02-12T19:09:49.756501Z","shell.execute_reply":"2026-02-12T19:09:49.759997Z"}},"outputs":[],"execution_count":17},{"id":"78aec578-7cc5-4c45-aaf2-7deb5be94cc3","cell_type":"markdown","source":"## Setup aima-python (optional)\n\nWenn du aima-python lokal oder auf Kaggle nutzen willst, kannst du das Repo installieren. Falls Installation nicht möglich ist, überspringe diesen Block und nutze die Minimal-Demo darunter.","metadata":{}},{"id":"c9c4a0b1-4d89-43f4-9277-8896b765c1a4","cell_type":"code","source":"!git clone --depth 1 https://github.com/aimacode/aima-python.git\n%cd aima-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:09:52.098891Z","iopub.execute_input":"2026-02-12T19:09:52.099175Z","iopub.status.idle":"2026-02-12T19:09:53.652281Z","shell.execute_reply.started":"2026-02-12T19:09:52.099152Z","shell.execute_reply":"2026-02-12T19:09:53.651179Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'aima-python'...\nremote: Enumerating objects: 214, done.\u001b[K\nremote: Counting objects: 100% (214/214), done.\u001b[K\nremote: Compressing objects: 100% (198/198), done.\u001b[K\nremote: Total 214 (delta 21), reused 114 (delta 12), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (214/214), 7.37 MiB | 18.91 MiB/s, done.\nResolving deltas: 100% (21/21), done.\n/kaggle/working/aima-python/aima-python/aima-python\n","output_type":"stream"}],"execution_count":18},{"id":"16150661-dec3-4fd3-a91b-592d22f2585f","cell_type":"code","source":"import sys, os\nsys.path.append(os.getcwd())  # damit aima-python Module importierbar sind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:09:56.504747Z","iopub.execute_input":"2026-02-12T19:09:56.505019Z","iopub.status.idle":"2026-02-12T19:09:56.509046Z","shell.execute_reply.started":"2026-02-12T19:09:56.504995Z","shell.execute_reply":"2026-02-12T19:09:56.508435Z"}},"outputs":[],"execution_count":19},{"id":"1ecc3727-28a0-40c8-bbc4-92c3457f3e73","cell_type":"markdown","source":"## Agent und Umgebung als Minimalmodell\n\nEin einfacher Reflex-Agent reagiert nur auf die aktuelle Wahrnehmung.","metadata":{}},{"id":"54d0ac6d-61b5-47bc-bac7-15ac9464032e","cell_type":"markdown","source":"### Pseudocode: Minimaler Agent in einer 1D-Umgebung\n\n**Idee:** Ein Agent bewegt sich in einer eindimensionalen Welt mit Feldern `0 ... size-1`.  \nZiel ist ein bestimmtes Feld `goal`. Pro Schritt gibt es eine kleine Strafe, beim Ziel eine Belohnung.\n\n**Umgebung (Environment)**\n1. Setze Startzustand: `pos = 0`\n2. Wiederhole für jeden Schritt:\n   - Nimm eine Aktion `action` entgegen (links oder rechts)\n   - Aktualisiere Position: `pos = pos + action`\n   - Begrenze Position auf gültige Felder: `pos ∈ [0, size-1]`\n   - Wenn `pos == goal`:\n     - `reward = 1`\n     - `done = True`\n   - Sonst:\n     - `reward = step_penalty` (z. B. `-0.01`)\n     - `done = False`\n   - Gib zurück: `(pos, reward, done)`\n\n**Agent (Reflex-Agent)**\n- Nutzt nur die aktuelle Beobachtung `obs`\n- Keine Erinnerung, kein Lernen\n- Regel: „Gehe immer nach rechts“\n\n**Episode (Simulationslauf)**\n1. `obs = env.reset()`\n2. `total = 0`\n3. Für maximal `T` Schritte:\n   - `action = agent(obs)`\n   - `(obs, reward, done) = env.step(action)`\n   - `total += reward`\n   - Wenn `done`: stoppe\n4. Ergebnis: `total` (Gesamtreward) und finale Position `obs`","metadata":{}},{"id":"05f48be3-9f56-4ca0-8008-5044793b12f3","cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass SimpleEnv:\n    \"\"\"\n    Minimal-Umgebung (1D):\n    - Zustände sind Positionen auf einer Linie: 0, 1, ..., size-1\n    - Der Agent startet immer bei pos=0\n    - Das Ziel ist ein bestimmtes Feld 'goal'\n    \"\"\"\n    size: int = 5     # Anzahl der Felder in der Welt\n    goal: int = 4     # Zielposition (muss zwischen 0 und size-1 liegen)\n\n    def reset(self):\n        \"\"\"\n        Startet eine neue Episode.\n        Setzt die Position auf den Startzustand und liefert die erste Beobachtung zurück.\n        \"\"\"\n        self.pos = 0\n        return self.pos  # Beobachtung = aktuelle Position\n\n    def step(self, action):\n        \"\"\"\n        Führt einen Zeitschritt aus.\n        action: -1 bedeutet \"nach links\", +1 bedeutet \"nach rechts\"\n\n        Rückgabe:\n        - neue Beobachtung (pos)\n        - reward (Belohnung/Strafe)\n        - done (True, wenn Episode beendet ist)\n        \"\"\"\n        # 1) Positionsupdate durch Aktion\n        # Beispiel: pos=2, action=+1 -> pos wird 3\n        self.pos = self.pos + action\n\n        # 2) Begrenzung auf gültige Felder (Randbedingungen):\n        # - unter 0 fällt der Agent nicht (bleibt bei 0)\n        # - über size-1 steigt er nicht (bleibt bei size-1)\n        self.pos = max(0, min(self.size - 1, self.pos))\n\n        # 3) Reward-Definition:\n        # - Ziel erreicht: +1\n        # - sonst: kleine Schrittstrafe (damit kurze Wege \"besser\" sind)\n        reward = 1 if self.pos == self.goal else -0.01\n\n        # 4) Abbruchbedingung:\n        # Episode endet, sobald das Ziel erreicht ist\n        done = (self.pos == self.goal)\n\n        return self.pos, reward, done\n\n\ndef reflex_agent(obs):\n    \"\"\"\n    Reflex-Agent:\n    - nutzt nur die aktuelle Beobachtung (obs), hier: Position\n    - hat kein Gedächtnis und lernt nicht\n    - Regel: immer nach rechts gehen\n    \"\"\"\n    return +1\n\n\n# --- Simulation einer Episode ---\nenv = SimpleEnv(size=5, goal=4)  # Umgebung erzeugen\nobs = env.reset()                # Episode starten, obs=Startposition\ntotal = 0                        # Gesamtreward sammeln\n\nfor t in range(20):              # Sicherheitslimit: max. 20 Schritte\n    action = reflex_agent(obs)   # Agent wählt Aktion basierend auf obs\n    obs, r, done = env.step(action)  # Umgebung führt Aktion aus\n    total += r                   # Reward aufaddieren\n\n    if done:                     # Ziel erreicht -> Episode beenden\n        break\n\ntotal, obs  # Ausgabe: (Gesamtreward, Endposition)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:58:37.718349Z","iopub.execute_input":"2026-02-12T19:58:37.718680Z","iopub.status.idle":"2026-02-12T19:58:37.729288Z","shell.execute_reply.started":"2026-02-12T19:58:37.718657Z","shell.execute_reply":"2026-02-12T19:58:37.728545Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(0.97, 4)"},"metadata":{}}],"execution_count":37},{"id":"6894284e-2cde-40fc-8c4e-eb389b24d9f9","cell_type":"markdown","source":"## Play-Zelle","metadata":{}},{"id":"c301f138-5ee2-49c2-8481-eb9f9e3590e3","cell_type":"code","source":"try:\n    import ipywidgets as w\n    from IPython.display import display, clear_output\nexcept Exception as e:\n    print(\"ipywidgets nicht verfügbar:\", e)\n    raise\n\nimport random\nfrom dataclasses import dataclass\n\n@dataclass\nclass SimpleEnvW:\n    size: int\n    goal: int\n    step_penalty: float\n\n    def reset(self):\n        self.pos = 0\n        return self.pos\n\n    def step(self, action):\n        self.pos = max(0, min(self.size-1, self.pos + action))\n        reward = 1.0 if self.pos == self.goal else self.step_penalty\n        done = self.pos == self.goal\n        return self.pos, reward, done\n\ndef run(p_random=0.2, env_size=7, step_penalty=-0.02, episodes=200):\n    env = SimpleEnvW(size=env_size, goal=env_size-1, step_penalty=step_penalty)\n\n    def agent(obs):\n        if random.random() < p_random:\n            return random.choice([-1, +1])\n        return +1\n\n    rets=[]\n    for _ in range(episodes):\n        s=env.reset()\n        total=0.0\n        for t in range(60):\n            a=agent(s)\n            s,r,done=env.step(a)\n            total += r\n            if done: break\n        rets.append(total)\n\n    clear_output(wait=True)\n    display(pd.Series(rets).describe())\n    plt.figure()\n    plt.hist(rets, bins=20)\n    plt.xlabel(\"Return\")\n    plt.ylabel(\"Häufigkeit\")\n    plt.show()\n\np = w.FloatSlider(value=0.2, min=0.0, max=1.0, step=0.05, description=\"p_random\")\nn = w.IntSlider(value=7, min=5, max=15, step=1, description=\"env_size\")\nsp = w.FloatSlider(value=-0.02, min=-0.2, max=0.0, step=0.01, description=\"penalty\")\nep = w.IntSlider(value=200, min=50, max=800, step=50, description=\"episodes\")\n\nui = w.VBox([p, n, sp, ep])\nout = w.interactive_output(run, {\"p_random\": p, \"env_size\": n, \"step_penalty\": sp, \"episodes\": ep})\ndisplay(ui, out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:00:28.698351Z","iopub.execute_input":"2026-02-12T20:00:28.698838Z","iopub.status.idle":"2026-02-12T20:00:28.833223Z","shell.execute_reply.started":"2026-02-12T20:00:28.698815Z","shell.execute_reply":"2026-02-12T20:00:28.832147Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(FloatSlider(value=0.2, description='p_random', max=1.0, step=0.05), IntSlider(value=7, descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e7ff42ed9b9473db9a75bf3c1a7d5a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1d344d4755425faed2c775a32d4fdc"}},"metadata":{}}],"execution_count":38},{"id":"fef71811-1b80-4a83-873b-72c6ae67654e","cell_type":"markdown","source":"### Interpretation der Skala und Parameterwirkung\n\n- **Return pro Episode** ist die Summe aller Rewards innerhalb einer Episode. Die Skala ergibt sich aus **Zielreward (typisch 1.0)** plus den **Schrittstrafen** bis zum Erreichen des Ziels.\n- **step_penalty (negativ)** verschiebt die Verteilung nach unten: Je stärker die Schrittstrafe, desto kleiner der Return, besonders wenn viele Schritte benötigt werden.\n- **env_size** bestimmt die Mindestzahl an Schritten bis zum Ziel (hier ist das Ziel `env_size - 1`). Größere Umgebungen führen bei gleicher Strategie im Mittel zu mehr Schritten und damit zu niedrigeren Returns.\n- **p_random** erhöht zufällige Aktionen. Dadurch steigt meist die Zahl der Schritte und die Varianz der Returns, weil der Agent häufiger Umwege geht oder vom Ziel weg bewegt wird.\n- **episodes** beeinflusst nur die Stabilität der Statistik: mehr Episoden ergeben eine glattere, zuverlässigere Verteilung, ändern aber nicht das Verhalten einer einzelnen Episode.\n- **max_steps** setzt eine Obergrenze: Wenn das Ziel nicht rechtzeitig erreicht wird, endet die Episode trotzdem. Viele Abbrüche führen zu einer Häufung niedriger Returns.","metadata":{}},{"id":"6588bcac-1de3-4e40-9dda-42ecc839a079","cell_type":"code","source":"import time\nimport random\nimport ipywidgets as w\nfrom IPython.display import display\n\ndef render_grid(pos, goal, width, height, walls=None, trail=None):\n    walls = walls or set()\n    trail = trail or set()\n    px, py = pos\n    gx, gy = goal\n\n    lines = []\n    for y in range(height):\n        row = []\n        for x in range(width):\n            cell = \".\"\n            if (x, y) in walls:\n                cell = \"#\"\n            if (x, y) in trail:\n                cell = \"*\"\n            if (x, y) == (gx, gy):\n                cell = \"G\"\n            if (x, y) == (px, py):\n                cell = \"M\"\n            row.append(cell)\n        lines.append(\" \".join(row))\n    return \"\\n\".join(lines)\n\ndef manhattan(a, b):\n    return abs(a[0]-b[0]) + abs(a[1]-b[1])\n\ndef generate_world(width, height, density=0.18, seed=0):\n    \"\"\"\n    Erzeugt ein Hindernisfeld mit garantierter Verbindung zwischen Start und Ziel:\n    - Zuerst wird ein Pfad gebaut, dann werden zufällige Wände gesetzt (nicht auf dem Pfad).\n    \"\"\"\n    rng = random.Random(seed)\n    start = (0, 0)\n    goal = (width-1, height-1)\n\n    # Pfad grob: rechts/runter mit kleinen Zufallsvarianten\n    path = {start}\n    x, y = start\n    while (x, y) != goal:\n        moves = []\n        if x < goal[0]: moves.append((x+1, y))\n        if y < goal[1]: moves.append((x, y+1))\n        # etwas Variation: manchmal zuerst runter statt rechts\n        nxt = rng.choice(moves)\n        x, y = nxt\n        path.add((x, y))\n\n    walls = set()\n    for yy in range(height):\n        for xx in range(width):\n            if (xx, yy) in path:\n                continue\n            if (xx, yy) == start or (xx, yy) == goal:\n                continue\n            if rng.random() < density:\n                walls.add((xx, yy))\n\n    return start, goal, walls\n\ndef run_grid_visual(out, width=18, height=10, density=0.18, seed=0,\n                    p_random=0.15, step_penalty=-0.02, max_steps=400, delay=0.04,\n                    show_trail=True):\n    # Welt generieren\n    start, goal, walls = generate_world(width, height, density=density, seed=seed)\n\n    def valid(p):\n        x, y = p\n        return 0 <= x < width and 0 <= y < height and p not in walls\n\n    actions = [(1,0), (-1,0), (0,1), (0,-1)]\n\n    def greedy_towards_goal(pos):\n        # wähle eine Aktion, die Manhattan-Distanz reduziert (wenn möglich)\n        best = []\n        best_d = manhattan(pos, goal)\n        for dx, dy in actions:\n            nxt = (pos[0]+dx, pos[1]+dy)\n            if not valid(nxt):\n                continue\n            d = manhattan(nxt, goal)\n            if d < best_d:\n                best = [(dx, dy)]\n                best_d = d\n            elif d == best_d:\n                best.append((dx, dy))\n        # fallback: irgendein valider Schritt\n        if best:\n            return random.choice(best)\n        valid_actions = [(dx,dy) for dx,dy in actions if valid((pos[0]+dx, pos[1]+dy))]\n        return random.choice(valid_actions) if valid_actions else (0,0)\n\n    pos = start\n    total = 0.0\n    trail = set()\n\n    with out:\n        out.clear_output(wait=True)\n        for t in range(max_steps):\n            if show_trail:\n                trail.add(pos)\n\n            out.clear_output(wait=True)\n            print(f\"t={t:03d}  pos={pos}  goal={goal}  return={total:.2f}  walls={len(walls)}\")\n            print(render_grid(pos, goal, width, height, walls, trail if show_trail else None))\n\n            # Aktion wählen\n            if random.random() < p_random:\n                dx, dy = random.choice(actions)\n            else:\n                dx, dy = greedy_towards_goal(pos)\n\n            nxt = (pos[0]+dx, pos[1]+dy)\n            if valid(nxt):\n                pos = nxt\n\n            # Reward\n            if pos == goal:\n                total += 1.0\n                out.clear_output(wait=True)\n                print(f\"Ziel erreicht in {t+1} Schritten, return={total:.2f}\")\n                print(render_grid(pos, goal, width, height, walls, trail if show_trail else None))\n                break\n            else:\n                total += step_penalty\n\n            time.sleep(delay)\n\n# UI\nbtn = w.Button(description=\"Run episode\", button_style=\"primary\")\nW = w.IntSlider(value=20, min=10, max=40, step=1, description=\"width\")\nH = w.IntSlider(value=10, min=6, max=20, step=1, description=\"height\")\nD = w.FloatSlider(value=0.18, min=0.05, max=0.35, step=0.01, description=\"density\")\nS = w.IntSlider(value=0, min=0, max=50, step=1, description=\"seed\")\n\nP = w.FloatSlider(value=0.15, min=0.0, max=1.0, step=0.05, description=\"p_random\")\nSP = w.FloatSlider(value=-0.02, min=-0.2, max=0.0, step=0.01, description=\"penalty\")\nDL = w.FloatSlider(value=0.04, min=0.01, max=0.2, step=0.01, description=\"delay\")\nTR = w.Checkbox(value=True, description=\"trail\")\n\nout = w.Output()\ndisplay(w.VBox([\n    w.HBox([btn]),\n    w.HBox([W, H, D, S]),\n    w.HBox([P, SP, DL, TR]),\n    out\n]))\n\ndef on_click(_):\n    run_grid_visual(\n        out,\n        width=W.value, height=H.value, density=D.value, seed=S.value,\n        p_random=P.value, step_penalty=SP.value, delay=DL.value,\n        show_trail=TR.value\n    )\n\nbtn.on_click(on_click)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:02:16.679908Z","iopub.execute_input":"2026-02-12T20:02:16.680165Z","iopub.status.idle":"2026-02-12T20:02:16.722203Z","shell.execute_reply.started":"2026-02-12T20:02:16.680144Z","shell.execute_reply":"2026-02-12T20:02:16.721303Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='primary', description='Run episode', style=ButtonStyle()),)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ea5bec5b5d48c09eae138bb0767a97"}},"metadata":{}}],"execution_count":39},{"id":"47efb1e8-c3f3-47dd-8598-7523b14ea96c","cell_type":"markdown","source":"### Interpretation der Skala und Parameterwirkung\n\n- **Return** ist die aufsummierte Belohnung pro Episode: Bei Zielerreichung gibt es typischerweise **+1.0**, ansonsten wird pro Schritt eine **Schrittstrafe** addiert. Viele Schritte bedeuten deshalb meist einen deutlich kleineren Return.\n- **step_penalty** (negativ) verschiebt die gesamte Episode nach unten: Je stärker die Schrittstrafe, desto stärker sinkt der Return bei Umwegen, Schleifen oder langsamer Zielerreichung.\n- **width, height** erhöhen die Problemgröße: längere Wege und mehr mögliche Zustände. Ohne zusätzliche Intelligenz steigt die Chance auf Umwege und damit sinkt der Return im Mittel.\n- **density** steuert die Hindernisdichte: mehr Wände bedeuten mehr Sackgassen und Umwege. Hohe Dichte erhöht typischerweise die Varianz der Returns, weil manche Läufe schnell eine gute Route finden und andere hängen bleiben.\n- **seed** erzeugt ein anderes Layout: Damit lassen sich Szenarien reproduzieren oder gezielt variieren, ohne die Parameter zu verändern.\n- **p_random** erhöht Zufallsschritte: Das kann helfen, lokale Blockaden zu lösen, führt aber bei zu hohem Wert zu ungerichtetem Verhalten, mehr Schritten und geringerem Return.\n- **trail** zeigt die Spur des Agenten: Gut, um Umwege, Schleifen und „Festhängen“ sichtbar zu machen. Der Trail verändert nicht das Verhalten, nur die Visualisierung.\n- **delay** beeinflusst nur die Darstellungsgeschwindigkeit, nicht das Entscheidungsverhalten.","metadata":{}},{"id":"6a5f1e39-4636-4be3-a6fe-c17fa9f492ec","cell_type":"code","source":"# Mini-Gridworld (Agenten-Notebook -> später 1:1 für Q-Learning im RL-Notebook)\n# Ziel: gleiche API wie in RL: reset() -> state, step(action) -> next_state, reward, done, info\n\nimport random\nimport numpy as np\n\nclass MiniGridworld:\n    \"\"\"\n    Zustände: (x, y) als Tupel\n    Aktionen: 0=up, 1=right, 2=down, 3=left\n    Rewards: step_penalty pro Schritt, goal_reward beim Ziel\n    Hindernisse: walls als Set[(x,y)]\n    \"\"\"\n    ACTIONS = {\n        0: (0, -1),  # up\n        1: (1, 0),   # right\n        2: (0, 1),   # down\n        3: (-1, 0),  # left\n    }\n    ACTION_NAMES = {0:\"up\", 1:\"right\", 2:\"down\", 3:\"left\"}\n\n    def __init__(self, width=8, height=6, start=(0,0), goal=(7,5),\n                 step_penalty=-0.02, goal_reward=1.0, max_steps=200,\n                 walls=None, seed=0):\n        self.width = width\n        self.height = height\n        self.start = start\n        self.goal = goal\n        self.step_penalty = step_penalty\n        self.goal_reward = goal_reward\n        self.max_steps = max_steps\n        self.walls = set(walls) if walls else set()\n        self.rng = random.Random(seed)\n\n        # Sicherheitschecks\n        assert self._in_bounds(self.start) and self.start not in self.walls\n        assert self._in_bounds(self.goal) and self.goal not in self.walls\n\n        self.reset()\n\n    def _in_bounds(self, p):\n        x, y = p\n        return 0 <= x < self.width and 0 <= y < self.height\n\n    def reset(self):\n        self.pos = self.start\n        self.t = 0\n        return self.pos\n\n    def step(self, action):\n        self.t += 1\n        dx, dy = self.ACTIONS[action]\n        nxt = (self.pos[0] + dx, self.pos[1] + dy)\n\n        # Wenn ungültig oder Wand: bleiben\n        if (not self._in_bounds(nxt)) or (nxt in self.walls):\n            nxt = self.pos\n\n        self.pos = nxt\n\n        done = False\n        reward = self.step_penalty\n\n        if self.pos == self.goal:\n            reward = self.goal_reward\n            done = True\n\n        if self.t >= self.max_steps:\n            done = True\n\n        info = {\"t\": self.t}\n        return self.pos, reward, done, info\n\n    def render(self, trail=None):\n        trail = trail or set()\n        lines = []\n        for y in range(self.height):\n            row = []\n            for x in range(self.width):\n                p = (x, y)\n                c = \".\"\n                if p in self.walls: c = \"#\"\n                if p in trail: c = \"*\"\n                if p == self.goal: c = \"G\"\n                if p == self.pos: c = \"M\"\n                row.append(c)\n            lines.append(\" \".join(row))\n        return \"\\n\".join(lines)\n\n    def state_id(self, state=None):\n        \"\"\"\n        Für Q-Learning praktisch: mappe (x,y) -> int [0..width*height-1]\n        \"\"\"\n        if state is None:\n            state = self.pos\n        x, y = state\n        return y * self.width + x\n\n    def id_to_state(self, sid):\n        x = sid % self.width\n        y = sid // self.width\n        return (x, y)\n\n\n# 1) Beispielwelt, die später exakt so im RL-Notebook wiederverwendet werden kann\nwalls = {\n    (2,1),(2,2),(2,3),\n    (4,2),(5,2),\n    (6,0),(6,1)\n}\n\nenv = MiniGridworld(\n    width=10, height=6,\n    start=(0,0), goal=(9,5),\n    step_penalty=-0.02, goal_reward=1.0,\n    max_steps=120,\n    walls=walls,\n    seed=0\n)\n\nprint(env.render())\n\n# 2) Kurzer \"Agentenlauf\" (greedy + etwas Zufall), nur zur Visualisierung\ndef greedy_action(state, goal):\n    # wählt eine Aktion, die Manhattan-Distanz reduziert, sonst zufällig\n    x, y = state\n    gx, gy = goal\n    candidates = []\n    if gy < y: candidates.append(0)  # up\n    if gx > x: candidates.append(1)  # right\n    if gy > y: candidates.append(2)  # down\n    if gx < x: candidates.append(3)  # left\n    return random.choice(candidates) if candidates else random.choice([0,1,2,3])\n\np_random = 0.2\ndelay = 0.08  # nur relevant, wenn du später sleep + clear_output nutzt\ntrail = set()\n\nstate = env.reset()\ntotal = 0.0\ndone = False\n\n# Ausgabe ohne Animation (nur \"Trace\" als Text)\nfor _ in range(40):\n    trail.add(state)\n    if random.random() < p_random:\n        action = random.choice([0,1,2,3])\n    else:\n        action = greedy_action(state, env.goal)\n\n    state, r, done, info = env.step(action)\n    total += r\n    if done:\n        break\n\nprint(\"\\nReturn:\", round(total, 3), \"Steps:\", info[\"t\"])\nprint(env.render(trail=trail))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:45:53.672341Z","iopub.execute_input":"2026-02-12T19:45:53.672653Z","iopub.status.idle":"2026-02-12T19:45:53.693711Z","shell.execute_reply.started":"2026-02-12T19:45:53.672627Z","shell.execute_reply":"2026-02-12T19:45:53.692996Z"}},"outputs":[{"name":"stdout","text":"M . . . . . # . . .\n. . # . . . # . . .\n. . # . # # . . . .\n. . # . . . . . . .\n. . . . . . . . . .\n. . . . . . . . . G\n\nReturn: 0.28 Steps: 37\n* . . . . . # . . .\n* * # . . . # . . .\n. * # . # # . . . .\n. * # . . . . . . .\n. * . . . . . . . .\n. * * * * * * * * M\n","output_type":"stream"}],"execution_count":34},{"id":"ffdeed94-9d9e-494a-a822-d60ed0ba06f3","cell_type":"code","source":"import time\nimport random\nimport ipywidgets as w\nfrom IPython.display import display\n\n# --- Hilfen: zufällige Wände mit garantiert freiem Start/Ziel ---\ndef generate_random_walls(width, height, start, goal, density=0.18, seed=0):\n    rng = random.Random(seed)\n    walls=set()\n    for y in range(height):\n        for x in range(width):\n            p=(x,y)\n            if p == start or p == goal:\n                continue\n            if rng.random() < density:\n                walls.add(p)\n    return walls\n\ndef choose_action_random():\n    return random.choice([0,1,2,3])\n\ndef choose_action_greedy(state, goal):\n    x, y = state\n    gx, gy = goal\n    preferred = []\n    if gy < y: preferred.append(0)  # up\n    if gx > x: preferred.append(1)  # right\n    if gy > y: preferred.append(2)  # down\n    if gx < x: preferred.append(3)  # left\n    return random.choice(preferred) if preferred else random.choice([0,1,2,3])\n\ndef run_episode(out,\n                width=12, height=8, density=0.18, seed=0,\n                step_penalty=-0.02, goal_reward=1.0,\n                policy=\"greedy\", p_random=0.2,\n                max_steps=200, delay=0.06, show_trail=True):\n\n    start=(0,0)\n    goal=(width-1, height-1)\n    walls = generate_random_walls(width, height, start, goal, density=density, seed=seed)\n\n    # Env neu erzeugen (wichtig: 1:1 gleiche API später für Q-Learning)\n    env = MiniGridworld(\n        width=width, height=height,\n        start=start, goal=goal,\n        step_penalty=step_penalty, goal_reward=goal_reward,\n        max_steps=max_steps,\n        walls=walls,\n        seed=seed\n    )\n\n    state = env.reset()\n    total = 0.0\n    trail = set()\n\n    with out:\n        out.clear_output(wait=True)\n\n        for t in range(max_steps):\n            if show_trail:\n                trail.add(state)\n\n            out.clear_output(wait=True)\n            print(f\"t={t:03d}  state={state}  return={total:.2f}  walls={len(walls)}\")\n            print(env.render(trail=trail if show_trail else None))\n\n            # Aktion wählen\n            if policy == \"random\":\n                action = choose_action_random()\n            else:\n                if random.random() < p_random:\n                    action = choose_action_random()\n                else:\n                    action = choose_action_greedy(state, env.goal)\n\n            state, r, done, info = env.step(action)\n            total += r\n\n            time.sleep(delay)\n            if done:\n                out.clear_output(wait=True)\n                print(f\"Ende: steps={info['t']}  return={total:.2f}  reached_goal={state==env.goal}  walls={len(walls)}\")\n                print(env.render(trail=trail if show_trail else None))\n                break\n\n# --- UI ---\nbtn = w.Button(description=\"Run episode\", button_style=\"primary\")\n\nW = w.IntSlider(value=20, min=10, max=40, step=1, description=\"width\")\nH = w.IntSlider(value=10, min=5, max=20, step=1, description=\"height\")\nD = w.FloatSlider(value=0.18, min=0.00, max=0.40, step=0.02, description=\"density\")\nS = w.IntSlider(value=0, min=0, max=99, step=1, description=\"seed\")\n\nSP = w.FloatSlider(value=-0.02, min=-0.30, max=0.00, step=0.01, description=\"penalty\")\nGR = w.FloatSlider(value=1.00, min=0.20, max=2.00, step=0.10, description=\"goal_reward\")\n\npolicy = w.Dropdown(options=[(\"Greedy (mit Zufall)\", \"greedy\"), (\"Nur Zufall\", \"random\")],\n                    value=\"greedy\", description=\"policy\")\nP = w.FloatSlider(value=0.20, min=0.0, max=1.0, step=0.05, description=\"p_random\")\n\nMS = w.IntSlider(value=200, min=30, max=600, step=10, description=\"max_steps\")\nDL = w.FloatSlider(value=0.06, min=0.01, max=0.30, step=0.01, description=\"delay\")\nTR = w.Checkbox(value=True, description=\"trail\")\n\nout = w.Output()\n\ndisplay(w.VBox([\n    w.HBox([btn]),\n    w.HBox([W, H, D, S]),\n    w.HBox([SP, GR]),\n    w.HBox([policy, P]),\n    w.HBox([MS, DL, TR]),\n    out\n]))\n\ndef on_click(_):\n    run_episode(\n        out,\n        width=W.value, height=H.value, density=D.value, seed=S.value,\n        step_penalty=SP.value, goal_reward=GR.value,\n        policy=policy.value, p_random=P.value,\n        max_steps=MS.value, delay=DL.value, show_trail=TR.value\n    )\n\nbtn.on_click(on_click)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:49:27.610218Z","iopub.execute_input":"2026-02-12T19:49:27.610514Z","iopub.status.idle":"2026-02-12T19:49:27.652150Z","shell.execute_reply.started":"2026-02-12T19:49:27.610483Z","shell.execute_reply":"2026-02-12T19:49:27.651510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='primary', description='Run episode', style=ButtonStyle()),)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9316970ceaad4bbd80ee61678b9e176b"}},"metadata":{}}],"execution_count":36},{"id":"3e9ec1e8-3957-4c83-928f-653e5905c931","cell_type":"markdown","source":"### MiniGridworld: Bedeutung der Parameter\n\n- **width, height** bestimmen die Größe der Welt und damit die Mindestlänge des Weges zum Ziel. Größere Welten führen meist zu mehr Schritten und damit zu einem niedrigeren Return.\n- **density** steuert die Hindernisdichte. Höhere Dichte erzeugt mehr Umwege und Sackgassen, erhöht die Varianz der Läufe und senkt typischerweise den Return.\n- **seed** erzeugt ein reproduzierbares Layout. Gleiche Parameter mit gleichem Seed liefern dieselbe Welt, unterschiedliche Seeds liefern neue Szenarien.\n- **step_penalty** ist die Schrittstrafe. Je stärker negativ, desto „teurer“ sind Umwege und desto wichtiger wird ein kurzer Weg.\n- **goal_reward** ist die Belohnung beim Erreichen des Ziels. Höhere Werte heben die Returns insgesamt an, ändern aber nicht, dass Umwege durch die Schrittstrafe bestraft werden.\n- **policy und p_random** steuern das Verhalten. Greedy versucht in Richtung Ziel zu gehen, `p_random` fügt Zufallsschritte hinzu, was aus Sackgassen helfen kann, aber bei zu hohem Wert zu unnötigen Umwegen führt.\n- **max_steps** begrenzt die Episodenlänge. Wenn das Ziel nicht erreicht wird, endet die Episode dennoch, meist mit niedrigem Return.","metadata":{}},{"id":"91d5670c-1b0c-48d2-be7e-1e828557c9d0","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}