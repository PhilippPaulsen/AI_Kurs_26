{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3 – Reinforcement Learning\n",
        "\n",
        "MDP, TD-Update und Q-Learning in einem kleinen Beispiel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TD Update per Hand\n",
        "\n",
        "Zahlenbeispiel: Zielwert, TD-Fehler, Update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "r = 2\n",
        "gamma = 0.9\n",
        "V_s = 4.0\n",
        "V_sp = 5.0\n",
        "alpha = 0.1\n",
        "\n",
        "target = r + gamma * V_sp\n",
        "delta = target - V_s\n",
        "V_new = V_s + alpha * delta\n",
        "\n",
        "target, delta, V_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-Gridworld (1D) und Q Tabelle\n",
        "\n",
        "Q-Learning ohne externe RL-Library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "n_states = 5\n",
        "actions = [-1, +1]\n",
        "goal = 4\n",
        "\n",
        "def step(state, action):\n",
        "    ns = max(0, min(n_states-1, state + action))\n",
        "    reward = 1.0 if ns == goal else -0.01\n",
        "    done = (ns == goal)\n",
        "    return ns, reward, done\n",
        "\n",
        "Q = {(s,a): 0.0 for s in range(n_states) for a in actions}\n",
        "\n",
        "def eps_greedy(s, eps=0.2):\n",
        "    if random.random() < eps:\n",
        "        return random.choice(actions)\n",
        "    return max(actions, key=lambda a: Q[(s,a)])\n",
        "\n",
        "alpha = 0.5\n",
        "gamma = 0.9\n",
        "eps = 0.2\n",
        "\n",
        "returns=[]\n",
        "for ep in range(200):\n",
        "    s=0\n",
        "    total=0\n",
        "    for t in range(50):\n",
        "        a = eps_greedy(s, eps)\n",
        "        sp, r, done = step(s,a)\n",
        "        total += r\n",
        "        best_next = max(Q[(sp,ap)] for ap in actions)\n",
        "        Q[(s,a)] = Q[(s,a)] + alpha*(r + gamma*best_next - Q[(s,a)])\n",
        "        s = sp\n",
        "        if done:\n",
        "            break\n",
        "    returns.append(total)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.show()\n",
        "\n",
        "policy = {s: max(actions, key=lambda a: Q[(s,a)]) for s in range(n_states)}\n",
        "policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-Übungen\n",
        "\n",
        "1) gamma auf 0.3 setzen. 2) eps nach 100 Episoden reduzieren. 3) Schrittstrafe ändern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Experimente hier durchführen\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "title": "Notebook 3 – Reinforcement Learning"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}