{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 - NLP Grundlagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lernziele\n",
    "- Du vergleichst Bag of Words und TF-IDF auf derselben Textaufgabe.\n",
    "- Du verstehst Tokenisierung, Vokabularaufbau und Vektorisierung als Minimalmodell.\n",
    "- Du berechnest TF-IDF an einem kleinen Beispiel nachvollziehbar per Handformel.\n",
    "- Du testest Embeddings spielerisch und unterscheidest Wort- von Satzreprasentationen.\n",
    "- Du setzt einfache Erweiterungen fuer Klassifikation und Sentiment um.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Warm-up Spielzelle: Bag of Words und TF-IDF mit Aehnlichkeit\n",
    "- Mini-Korpus plus neuer Satz als Eingabe.\n",
    "- Methoden: Count oder TF-IDF.\n",
    "- Ausgabe: Top Features, Aehnlichkeitstabelle, TF-IDF-Heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "CORPUS = [\n",
    "    \"Der Roboter sortiert Pakete im Lager.\",\n",
    "    \"Das Expertensystem nutzt klare Regeln.\",\n",
    "    \"Maschinelles Lernen findet Muster in Daten.\",\n",
    "    \"Heute ist das Wetter warm und sonnig.\",\n",
    "    \"Wir ueben Textklassifikation mit Python.\",\n",
    "    \"TF-IDF gewichtet informative Begriffe staerker.\",\n",
    "    \"Bag of Words ignoriert oft Reihenfolge.\",\n",
    "    \"Der Hund rennt schnell durch den Garten.\",\n",
    "    \"Semantik ist mit reinen Wortzaehlungen schwer.\",\n",
    "    \"Kurze Saetze helfen beim Einstieg in NLP.\",\n",
    "]\n",
    "\n",
    "STOPWORDS_DE = {\n",
    "    \"der\", \"die\", \"das\", \"und\", \"ist\", \"im\", \"in\", \"mit\", \"wir\", \"den\", \"ein\", \"eine\",\n",
    "    \"heute\", \"oft\", \"durch\", \"zu\", \"von\", \"am\", \"an\", \"auf\", \"beim\"\n",
    "}\n",
    "\n",
    "print('Korpusgroesse:', len(CORPUS), 'Saetze')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_vectorizer(method, ngram_max, use_stopwords, min_df):\n",
    "    stop_words = list(STOPWORDS_DE) if use_stopwords else None\n",
    "    kwargs = {\n",
    "        'ngram_range': (1, ngram_max),\n",
    "        'min_df': min_df,\n",
    "        'stop_words': stop_words,\n",
    "    }\n",
    "    if method == 'Count':\n",
    "        return CountVectorizer(**kwargs)\n",
    "    return TfidfVectorizer(**kwargs)\n",
    "\n",
    "\n",
    "def top_features(matrix, feature_names, k=10):\n",
    "    scores = np.asarray(matrix.sum(axis=0)).ravel()\n",
    "    idx = np.argsort(scores)[::-1]\n",
    "    top_idx = idx[:min(k, len(idx))]\n",
    "    return [(feature_names[i], float(scores[i])) for i in top_idx if scores[i] > 0]\n",
    "\n",
    "\n",
    "def plot_tfidf_heatmap(feature_names, top_k=20):\n",
    "    # Fuer die Heatmap nehmen wir immer TF-IDF, auch wenn Count gewaehlt wurde.\n",
    "    tfidf = TfidfVectorizer(vocabulary=feature_names)\n",
    "    X_tfidf = tfidf.fit_transform(CORPUS)\n",
    "\n",
    "    importance = np.asarray(X_tfidf.sum(axis=0)).ravel()\n",
    "    idx = np.argsort(importance)[::-1][:min(top_k, len(importance))]\n",
    "    selected_features = [tfidf.get_feature_names_out()[i] for i in idx]\n",
    "    mat = X_tfidf[:, idx].toarray()\n",
    "\n",
    "    plt.figure(figsize=(10, 4.2))\n",
    "    im = plt.imshow(mat, aspect='auto', cmap='YlGnBu')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.yticks(range(len(CORPUS)), [f'D{i+1}' for i in range(len(CORPUS))])\n",
    "    plt.xticks(range(len(selected_features)), selected_features, rotation=45, ha='right')\n",
    "    plt.title('TF-IDF Matrix Heatmap (Korpus x Top Features)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "method_dd = widgets.Dropdown(options=['Count', 'TFIDF'], value='Count', description='method')\n",
    "ngram_sl = widgets.IntSlider(value=1, min=1, max=2, step=1, description='ngram_max', continuous_update=False)\n",
    "stop_cb = widgets.Checkbox(value=False, description='use_stopwords')\n",
    "min_df_sl = widgets.IntSlider(value=1, min=1, max=3, step=1, description='min_df', continuous_update=False)\n",
    "new_text = widgets.Text(value='Das System erkennt Muster in Textdaten.', description='neuer Satz', layout=widgets.Layout(width='95%'))\n",
    "run_btn = widgets.Button(description='Run warm-up', button_style='info')\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def run_warmup(_):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        vec = make_vectorizer(method_dd.value, ngram_sl.value, stop_cb.value, min_df_sl.value)\n",
    "\n",
    "        all_docs = CORPUS + [new_text.value.strip()]\n",
    "        X_all = vec.fit_transform(all_docs)\n",
    "        feature_names = vec.get_feature_names_out()\n",
    "\n",
    "        X_corpus = X_all[:-1]\n",
    "        x_new = X_all[-1]\n",
    "\n",
    "        tf = top_features(X_corpus, feature_names, k=10)\n",
    "        print('Top Features:')\n",
    "        if tf:\n",
    "            for token, val in tf:\n",
    "                print(f'- {token}: {val:.4f}')\n",
    "        else:\n",
    "            print('- Keine Features gefunden (min_df/stopwords pruefen).')\n",
    "\n",
    "        sims = cosine_similarity(x_new, X_corpus).ravel()\n",
    "        sim_df = pd.DataFrame({\n",
    "            'satz_id': [f'S{i+1}' for i in range(len(CORPUS))],\n",
    "            'satz': CORPUS,\n",
    "            'cosine_similarity': sims,\n",
    "        }).sort_values('cosine_similarity', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        print('\\nAehnlichkeit neuer Satz vs Korpus:')\n",
    "        display(sim_df)\n",
    "\n",
    "        plot_tfidf_heatmap(feature_names, top_k=20)\n",
    "\n",
    "\n",
    "run_btn.on_click(run_warmup)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    method_dd,\n",
    "    ngram_sl,\n",
    "    stop_cb,\n",
    "    min_df_sl,\n",
    "    new_text,\n",
    "    run_btn,\n",
    "    out,\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Minimalmodell und Pseudocode (Token, Vokabular, Vektor)\n",
    "- Tokenisierung zerlegt Saetze in Grundbausteine.\n",
    "- Das Vokabular sammelt alle beobachteten Begriffe.\n",
    "- Jeder Satz wird als Vektor ueber dem Vokabular dargestellt.\n",
    "\n",
    "```text\n",
    "INPUT: dokumente\n",
    "tokens_pro_doc <- tokenize(dokumente)\n",
    "vocab <- sort(unique(alle_tokens))\n",
    "FOR doc IN tokens_pro_doc\n",
    "  vec <- nullvektor(len(vocab))\n",
    "  FOR token IN doc\n",
    "    vec[index(vocab, token)] += 1   # oder TF-IDF-Gewicht\n",
    "RETURN vocab, matrix_aus_vektoren\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Minimalbeispiel mit explizitem Vokabular und Zaehlern.\n",
    "mini_docs = [\n",
    "    'katze mag milch',\n",
    "    'hund mag park',\n",
    "    'katze und hund'\n",
    "]\n",
    "\n",
    "# Sehr einfache Tokenisierung via split.\n",
    "tokens = [d.split() for d in mini_docs]\n",
    "vocab = sorted({t for doc in tokens for t in doc})\n",
    "\n",
    "rows = []\n",
    "for d, tok in zip(mini_docs, tokens):\n",
    "    vec = {v: 0 for v in vocab}\n",
    "    for t in tok:\n",
    "        vec[t] += 1\n",
    "    vec['doc'] = d\n",
    "    rows.append(vec)\n",
    "\n",
    "display(pd.DataFrame(rows)[['doc'] + vocab])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) TF-IDF Rechenbeispiel plus Visualisierung\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kleiner Datensatz mit 3 Dokumenten und 3 Zielbegriffen.\n",
    "docs = [\n",
    "    'ki modell daten daten',\n",
    "    'daten analyse statistik',\n",
    "    'modell lernen ki',\n",
    "]\n",
    "terms = ['ki', 'daten', 'modell']\n",
    "N = len(docs)\n",
    "\n",
    "rows = []\n",
    "for i, d in enumerate(docs, 1):\n",
    "    toks = d.split()\n",
    "    for term in terms:\n",
    "        tf = toks.count(term)\n",
    "        df = sum(1 for doc in docs if term in doc.split())\n",
    "        idf = np.log((N + 1) / (df + 1)) + 1\n",
    "        tfidf = tf * idf\n",
    "        rows.append({'doc': f'D{i}', 'term': term, 'tf': tf, 'df': df, 'idf': round(idf, 4), 'tfidf': round(tfidf, 4)})\n",
    "\n",
    "calc_df = pd.DataFrame(rows)\n",
    "display(calc_df)\n",
    "\n",
    "# Balkenplot: mittleres TF-IDF je Begriff.\n",
    "mean_tfidf = calc_df.groupby('term', as_index=False)['tfidf'].mean()\n",
    "plt.figure(figsize=(5.4, 3.5))\n",
    "plt.bar(mean_tfidf['term'], mean_tfidf['tfidf'], color='tab:blue')\n",
    "plt.title('Mittleres TF-IDF fuer 3 Begriffe')\n",
    "plt.ylabel('mean tfidf')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Embeddings Block: Word und Satz-Embeddings (spielbar)\n",
    "### BoW vs Embeddings\n",
    "- BoW/TF-IDF modelliert Woerter als diskrete Merkmale ohne tiefe Semantik.\n",
    "- Embeddings legen Woerter/Saetze in kontinuierliche Vektorraeume.\n",
    "- Semantisch aehnliche Begriffe liegen im Embeddingraum naeher beieinander.\n",
    "- Satz-Embeddings komprimieren ganze Saetze fuer Suche und Aehnlichkeitsvergleiche.\n",
    "- Kontext bleibt bei einfachen statischen Wortvektoren begrenzt.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Versuche leichte vortrainte Embeddings aus gensim zu laden.\n",
    "# Falls nicht verfuegbar: SVD auf TF-IDF als Pseudo-Embedding (Approximation).\n",
    "REAL_EMBEDDINGS = None\n",
    "EMBEDDING_BACKEND = 'pseudo_svd'\n",
    "\n",
    "try:\n",
    "    import gensim.downloader as api\n",
    "    REAL_EMBEDDINGS = api.load('glove-wiki-gigaword-50')\n",
    "    EMBEDDING_BACKEND = 'gensim_glove_50'\n",
    "except Exception:\n",
    "    REAL_EMBEDDINGS = None\n",
    "    EMBEDDING_BACKEND = 'pseudo_svd'\n",
    "\n",
    "print('Embedding Backend:', EMBEDDING_BACKEND)\n",
    "\n",
    "# Basis fuer Pseudo-Embeddings\n",
    "pseudo_texts = CORPUS + [\n",
    "    'robot', 'system', 'learning', 'data', 'dog', 'weather', 'text', 'python', 'rules', 'model'\n",
    "]\n",
    "pseudo_tfidf = TfidfVectorizer(min_df=1)\n",
    "X_pseudo = pseudo_tfidf.fit_transform(pseudo_texts)\n",
    "svd = TruncatedSVD(n_components=min(20, X_pseudo.shape[1]-1), random_state=0)\n",
    "X_emb = svd.fit_transform(X_pseudo)\n",
    "\n",
    "pseudo_index = {txt: i for i, txt in enumerate(pseudo_texts)}\n",
    "\n",
    "\n",
    "def get_word_vector(word):\n",
    "    w = word.strip().lower()\n",
    "    if REAL_EMBEDDINGS is not None and w in REAL_EMBEDDINGS:\n",
    "        return REAL_EMBEDDINGS[w]\n",
    "    if w in pseudo_index:\n",
    "        return X_emb[pseudo_index[w]]\n",
    "    return None\n",
    "\n",
    "\n",
    "def sentence_vector(sentence):\n",
    "    if REAL_EMBEDDINGS is not None:\n",
    "        toks = [t.lower().strip('.,!?') for t in sentence.split()]\n",
    "        vecs = [REAL_EMBEDDINGS[t] for t in toks if t in REAL_EMBEDDINGS]\n",
    "        if vecs:\n",
    "            return np.mean(vecs, axis=0)\n",
    "    # Fallback: SVD-Vektor aus TF-IDF-Satzdarstellung\n",
    "    mat = pseudo_tfidf.transform([sentence])\n",
    "    return svd.transform(mat)[0]\n",
    "\n",
    "\n",
    "def safe_cos(a, b):\n",
    "    if a is None or b is None:\n",
    "        return np.nan\n",
    "    return float(cosine_similarity([a], [b])[0, 0])\n",
    "\n",
    "\n",
    "def top5_similar_words(word):\n",
    "    if REAL_EMBEDDINGS is None:\n",
    "        return []\n",
    "    w = word.strip().lower()\n",
    "    if w not in REAL_EMBEDDINGS:\n",
    "        return []\n",
    "    return REAL_EMBEDDINGS.most_similar(w, topn=5)\n",
    "\n",
    "\n",
    "def plot_word_projection(words):\n",
    "    vecs = []\n",
    "    valid_words = []\n",
    "    for w in words:\n",
    "        v = get_word_vector(w)\n",
    "        if v is not None:\n",
    "            vecs.append(v)\n",
    "            valid_words.append(w)\n",
    "    if len(valid_words) < 2:\n",
    "        print('Zu wenige bekannte Woerter fuer Projektion.')\n",
    "        return\n",
    "\n",
    "    mat = np.vstack(vecs)\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    pts = pca.fit_transform(mat)\n",
    "\n",
    "    plt.figure(figsize=(6.2, 4.2))\n",
    "    plt.scatter(pts[:, 0], pts[:, 1], color='tab:blue')\n",
    "    for i, w in enumerate(valid_words):\n",
    "        plt.text(pts[i, 0], pts[i, 1], w)\n",
    "    plt.title('2D Projektion von Wortvektoren (PCA)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mode_dd = widgets.Dropdown(options=['word', 'sentence'], value='word', description='embedding_mode')\n",
    "word1_in = widgets.Text(value='robot', description='word1')\n",
    "word2_in = widgets.Text(value='system', description='word2')\n",
    "sent1_in = widgets.Text(value='Das System lernt aus Daten.', description='sentence1', layout=widgets.Layout(width='95%'))\n",
    "sent2_in = widgets.Text(value='Modelle erkennen Muster.', description='sentence2', layout=widgets.Layout(width='95%'))\n",
    "compare_btn = widgets.Button(description='Compare', button_style='info')\n",
    "emb_out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_compare(_):\n",
    "    with emb_out:\n",
    "        emb_out.clear_output()\n",
    "\n",
    "        if mode_dd.value == 'word':\n",
    "            w1, w2 = word1_in.value.strip(), word2_in.value.strip()\n",
    "            v1, v2 = get_word_vector(w1), get_word_vector(w2)\n",
    "            sim = safe_cos(v1, v2)\n",
    "            print(f'Word cosine similarity ({w1}, {w2}):', sim)\n",
    "\n",
    "            if REAL_EMBEDDINGS is not None:\n",
    "                sim_words = top5_similar_words(w1)\n",
    "                sim_df = pd.DataFrame(sim_words, columns=['word', 'score'])\n",
    "                print('\\nTop 5 aehnliche Woerter zu', w1)\n",
    "                display(sim_df)\n",
    "            else:\n",
    "                print('\\nTop-5 Wortnachbarn nur mit echten Embeddings verfuegbar (Fallback aktiv).')\n",
    "\n",
    "            plot_word_projection(['robot', 'system', 'learning', 'data', 'dog', 'weather', 'text', 'python', 'rules', 'model'])\n",
    "\n",
    "        else:\n",
    "            s1, s2 = sent1_in.value.strip(), sent2_in.value.strip()\n",
    "            v1, v2 = sentence_vector(s1), sentence_vector(s2)\n",
    "            sim = safe_cos(v1, v2)\n",
    "            print('Sentence cosine similarity:', sim)\n",
    "            print('Hinweis: Satz-Embeddings sind kompakte Repraesentationen ganzer Saetze, gut fuer Aehnlichkeit und Suche.')\n",
    "\n",
    "\n",
    "compare_btn.on_click(on_compare)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    mode_dd,\n",
    "    word1_in,\n",
    "    word2_in,\n",
    "    sent1_in,\n",
    "    sent2_in,\n",
    "    compare_btn,\n",
    "    emb_out,\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Aufsteigende Erweiterungen: Klassifikation, Sentiment, Themen\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Erweiterung 1: Mini-Klassifikation (2 Labels) auf TF-IDF + Logistic Regression.\n",
    "cls_texts = [\n",
    "    'Das Modell erkennt Muster gut.',\n",
    "    'Wir trainieren einen Klassifikator.',\n",
    "    'Der Hund spielt im Park.',\n",
    "    'Heute scheint die Sonne.',\n",
    "    'NLP nutzt Vektoren fuer Text.',\n",
    "    'Regeln koennen Entscheidungen erklaeren.',\n",
    "]\n",
    "cls_y = [1, 1, 0, 0, 1, 1]\n",
    "\n",
    "vec = TfidfVectorizer(min_df=1)\n",
    "X = vec.fit_transform(cls_texts)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, cls_y)\n",
    "pred = clf.predict(X)\n",
    "acc = (pred == np.array(cls_y)).mean()\n",
    "\n",
    "print('Mini-Klassifikation Accuracy (Train-Set-Demo):', round(float(acc), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Erweiterung 2: Mini-Sentiment mit handgemachtem Set.\n",
    "sent_texts = [\n",
    "    'Das Produkt ist grossartig und hilfreich.',\n",
    "    'Ich mag die klare Struktur.',\n",
    "    'Die Ergebnisse sind schlecht und verwirrend.',\n",
    "    'Das war eine tolle Erklaerung.',\n",
    "    'Ich bin unzufrieden mit der Qualitaet.',\n",
    "    'Der Ablauf war angenehm und schnell.',\n",
    "    'Das Update ist problematisch und langsam.',\n",
    "    'Sehr gute Dokumentation und Beispiele.',\n",
    "    'Ich finde es frustrierend.',\n",
    "    'Das Tool ist praktisch und stabil.',\n",
    "]\n",
    "sent_y = [1, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "sv = TfidfVectorizer(min_df=1)\n",
    "Xs = sv.fit_transform(sent_texts)\n",
    "mdl = LogisticRegression(max_iter=1000)\n",
    "mdl.fit(Xs, sent_y)\n",
    "pred_s = mdl.predict(Xs)\n",
    "acc_s = (pred_s == np.array(sent_y)).mean()\n",
    "\n",
    "res_df = pd.DataFrame({'text': sent_texts, 'true': sent_y, 'pred': pred_s})\n",
    "print('Mini-Sentiment Accuracy (Train-Set-Demo):', round(float(acc_s), 4))\n",
    "display(res_df.head(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erweiterung 3: Themen (Konzept)\n",
    "- Themenmodellierung gruppiert Dokumente ueber wiederkehrende Wortmuster.\n",
    "- Typische Verfahren sind LSA/NMF/LDA mit unterschiedlichen Annahmen.\n",
    "- Fuer dieses Notebook bleibt es beim Konzept, um Rechenaufwand klein zu halten.\n",
    "- In der Praxis folgt nach Themen oft eine manuelle Label-Interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Mini Leitfaden (7 bis 10 Minuten)\n",
    "- Minute 0-1: Lernziele lesen und Korpus ueberfliegen.\n",
    "- Minute 1-3: Warm-up mit Count starten, Aehnlichkeitstabelle lesen.\n",
    "- Minute 3-5: auf TF-IDF wechseln und Heatmap vergleichen.\n",
    "- Minute 5-7: TF-IDF Mini-Rechnung durchgehen und Balkenplot interpretieren.\n",
    "- Minute 7-9: Embeddings-Block in word und sentence mode testen.\n",
    "- Minute 9-10: Erweiterungen kurz laufen lassen und Grenzen notieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Uebungen\n",
    "1. Formuliere zwei neue Saetze, die inhaltlich aehnlich sind, aber unterschiedliche Woerter nutzen, und vergleiche BoW vs Embeddings.\n",
    "2. Erhoehe `min_df` und dokumentiere, welche Features aus der Heatmap verschwinden.\n",
    "3. Setze `ngram_max=2` und identifiziere ein hilfreiches Bigramm.\n",
    "4. Aendere im TF-IDF Rechenbeispiel ein Dokument und berechne den Effekt auf `idf` fuer `daten`.\n",
    "5. Ergaenze zwei Sentiment-Saetze und pruefe, wie stabil die Mini-Klassifikation bleibt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}