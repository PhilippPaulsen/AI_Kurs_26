{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f9d03aa7-d8ff-450e-8e60-eed463eff16e","cell_type":"markdown","source":"# Agenten und Reinforcement Learning in MiniGridworld\n\nDieses Notebook verbindet einfache Agenten mit Q Learning in einer gemeinsamen Umgebung. Die Beispiele bleiben kompakt und direkt ausführbar. Mini-Leitfaden siehe unten.","metadata":{}},{"id":"b46ba225-6e5e-4d85-8bb6-c60b0e6af3f8","cell_type":"markdown","source":"## Setup und Imports","metadata":{}},{"id":"750c41f3-f1fb-4b16-8bbb-20980eaab57e","cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\n\nWIDGETS_OK = True\nWIDGET_IMPORT_ERROR = None\ntry:\n    import ipywidgets as w\nexcept Exception as e:\n    WIDGETS_OK = False\n    WIDGET_IMPORT_ERROR = e\n    print('ipywidgets ist nicht verfügbar. Installiere es mit: pip install ipywidgets')\n    print('Fehler:', e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T11:12:33.142273Z","iopub.execute_input":"2026-02-13T11:12:33.143027Z","iopub.status.idle":"2026-02-13T11:12:33.148743Z","shell.execute_reply.started":"2026-02-13T11:12:33.142996Z","shell.execute_reply":"2026-02-13T11:12:33.147664Z"},"_kg_hide-input":true},"outputs":[],"execution_count":37},{"id":"9292431b-608c-4ca9-8bbc-c16aefad0883","cell_type":"markdown","source":"## MiniGridworld und make_env\n- MiniGridworld ist eine einfache 2D Testumgebung mit Feldern, Start und Ziel.\n- Der Agent startet links oben und soll das Ziel rechts unten erreichen.\n- Hindernisse blockieren Wege, der Agent muss einen begehbaren Pfad finden.\n- Pro Schritt wählt der Agent eine Aktion: hoch, rechts, runter, links.\n- Rewards entstehen durch Schrittstrafe und Zielbelohnung.\n- make_env erzeugt eine konkrete Welt mit Parametern wie width, height, density, seed, step_penalty, goal_reward, max_steps.\n- Dadurch sind Welten reproduzierbar und Parameteränderungen lassen sich fair vergleichen.\n- Wir beobachten dafür Erfolg, Schrittzahl und Return.","metadata":{}},{"id":"8d11edb1-ae1c-401b-a7b8-dada1fe60372","cell_type":"code","source":"class MiniGridworld:\n    # Aktionen: 0 up, 1 right, 2 down, 3 left\n    ACTIONS = {0: (0, -1), 1: (1, 0), 2: (0, 1), 3: (-1, 0)}\n\n    def __init__(self, width=20, height=10, start=(0, 0), goal=(9, 5),\n                 step_penalty=-0.02, goal_reward=1.0, max_steps=250,\n                 walls=None, seed=0):\n        self.width = int(width)\n        self.height = int(height)\n        self.start = tuple(start)\n        self.goal = tuple(goal)\n        self.step_penalty = float(step_penalty)\n        self.goal_reward = float(goal_reward)\n        self.max_steps = int(max_steps)\n        self.walls = set(walls) if walls else set()\n        self.seed = int(seed)\n\n        assert self._in_bounds(self.start) and self.start not in self.walls\n        assert self._in_bounds(self.goal) and self.goal not in self.walls\n\n        self.reset()\n\n    def _in_bounds(self, p):\n        x, y = p\n        return 0 <= x < self.width and 0 <= y < self.height\n\n    def reset(self):\n        self.pos = self.start\n        self.t = 0\n        return self.pos\n\n    def step(self, action):\n        self.t += 1\n        dx, dy = self.ACTIONS[int(action)]\n        nxt = (self.pos[0] + dx, self.pos[1] + dy)\n\n        if (not self._in_bounds(nxt)) or (nxt in self.walls):\n            nxt = self.pos\n\n        self.pos = nxt\n        done = False\n        reward = self.step_penalty\n\n        if self.pos == self.goal:\n            reward = self.goal_reward\n            done = True\n\n        if self.t >= self.max_steps:\n            done = True\n\n        return self.pos, reward, done, {'t': self.t}\n\n    def state_id(self, state=None):\n        s = self.pos if state is None else state\n        x, y = s\n        return y * self.width + x\n\n    def render(self, trail=None):\n        trail = trail or set()\n        lines = []\n        for y in range(self.height):\n            row = []\n            for x in range(self.width):\n                p = (x, y)\n                c = '.'\n                if p in self.walls:\n                    c = '#'\n                if p in trail:\n                    c = '*'\n                if p == self.goal:\n                    c = 'G'\n                if p == self.pos:\n                    c = 'M'\n                row.append(c)\n            lines.append(' '.join(row))\n        return '\\n'.join(lines)\n\n\ndef generate_random_walls(width, height, start, goal, density=0.18, seed=0):\n    rng = random.Random(seed)\n    walls = set()\n    for y in range(height):\n        for x in range(width):\n            p = (x, y)\n            if p == start or p == goal:\n                continue\n            if rng.random() < density:\n                walls.add(p)\n    return walls\n\n\ndef make_env(width=20, height=10, density=0.18, seed=0,\n             step_penalty=-0.02, goal_reward=1.0, max_steps=300):\n    start = (0, 0)\n    goal = (width - 1, height - 1)\n    walls = generate_random_walls(width, height, start, goal, density=density, seed=seed)\n    return MiniGridworld(width=width, height=height, start=start, goal=goal,\n                         step_penalty=step_penalty, goal_reward=goal_reward,\n                         max_steps=max_steps, walls=walls, seed=seed)\n\n\nenv_demo = make_env(width=12, height=8, density=0.20, seed=3)\nprint(env_demo.render())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T11:13:01.617687Z","iopub.execute_input":"2026-02-13T11:13:01.618535Z","iopub.status.idle":"2026-02-13T11:13:01.638264Z","shell.execute_reply.started":"2026-02-13T11:13:01.618504Z","shell.execute_reply":"2026-02-13T11:13:01.637078Z"}},"outputs":[{"name":"stdout","text":"M . . . . . # # . . . .\n. . . . # . . . . . # .\n. . # . . . . . . . . .\n. . # # . . . . . . . .\n. . . . . . . . # . . .\n. . . . . . # . . # . .\n# . . . # . # . . . . .\n. . # . # # . . # # . G\n","output_type":"stream"}],"execution_count":38},{"id":"afd63826-ad89-424a-9bbe-1f24c4a977a2","cell_type":"markdown","source":"## Baseline Agenten und kurze Evaluation\n- Random: wählt in jedem Schritt eine zufällige Aktion. Dient als untere Referenz ohne Zielorientierung.\n- Greedy: wählt die Aktion, die die Manhattan-Distanz zum Ziel am stärksten reduziert. Kein Lernen, kann in Sackgassen oder Schleifen hängen bleiben.\n- Greedy mit Memory: wie Greedy, aber meidet bereits besuchte Felder, um Wiederholungen zu reduzieren. Kein Lernen, aber oft stabiler als Greedy.","metadata":{}},{"id":"2098637a-307e-48a8-a944-12b158e4186b","cell_type":"code","source":"ACTIONS = [0, 1, 2, 3]\n\ndef manhattan(s, g):\n    return abs(s[0] - g[0]) + abs(s[1] - g[1])\n\n\ndef choose_action_random(state, env):\n    return random.choice(ACTIONS)\n\n\ndef choose_action_greedy(state, env):\n    x, y = state\n    gx, gy = env.goal\n    preferred = []\n    if gy < y:\n        preferred.append(0)\n    if gx > x:\n        preferred.append(1)\n    if gy > y:\n        preferred.append(2)\n    if gx < x:\n        preferred.append(3)\n    return random.choice(preferred) if preferred else random.choice(ACTIONS)\n\n\ndef choose_action_greedy_memory(state, env, memory):\n    # Vermeidet kurze Schleifen, indem zuletzt besuchte Zustände gemieden werden\n    candidates = []\n    for a in ACTIONS:\n        old_pos, old_t = env.pos, env.t\n        env.pos, env.t = state, 0\n        sp, _, _, _ = env.step(a)\n        env.pos, env.t = old_pos, old_t\n        candidates.append((a, manhattan(sp, env.goal), sp in memory))\n\n    candidates.sort(key=lambda x: (x[2], x[1]))\n    return int(candidates[0][0])\n\n\ndef run_baseline_episode(env, mode='random'):\n    s = env.reset()\n    total = 0.0\n    memory = [s]\n\n    for t in range(env.max_steps):\n        if mode == 'random':\n            a = choose_action_random(s, env)\n        elif mode == 'greedy':\n            a = choose_action_greedy(s, env)\n        elif mode == 'greedy_memory':\n            a = choose_action_greedy_memory(s, env, set(memory[-10:]))\n        else:\n            raise ValueError('Unbekannter mode')\n\n        s, r, done, info = env.step(a)\n        total += r\n        memory.append(s)\n        if done:\n            return total, info.get('t', t + 1), int(s == env.goal)\n\n    return total, env.max_steps, 0\n\n\ndef evaluate_baseline_modes(width=12, height=8, density=0.20, seed=3,\n                            step_penalty=-0.02, goal_reward=1.0, max_steps=250,\n                            episodes=50):\n    rows = []\n    for mode in ['random', 'greedy', 'greedy_memory']:\n        returns = []\n        steps = []\n        succ = 0\n        for _ in range(episodes):\n            env = make_env(width, height, density, seed, step_penalty, goal_reward, max_steps)\n            ret, st, ok = run_baseline_episode(env, mode=mode)\n            returns.append(ret)\n            steps.append(st)\n            succ += ok\n\n        rows.append({\n            'mode': mode,\n            'success_rate': succ / episodes,\n            'mean_steps': float(np.mean(steps)),\n            'mean_return': float(np.mean(returns))\n        })\n\n    return pd.DataFrame(rows).sort_values(['success_rate', 'mean_return'], ascending=[False, False])\n\n\ndisplay(evaluate_baseline_modes())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T11:13:25.387614Z","iopub.execute_input":"2026-02-13T11:13:25.388913Z","iopub.status.idle":"2026-02-13T11:13:25.448162Z","shell.execute_reply.started":"2026-02-13T11:13:25.388864Z","shell.execute_reply":"2026-02-13T11:13:25.447335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"            mode  success_rate  mean_steps  mean_return\n2  greedy_memory          1.00       18.00       0.6600\n1         greedy          0.36      169.18      -3.0164\n0         random          0.16      234.00      -4.5168","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mode</th>\n      <th>success_rate</th>\n      <th>mean_steps</th>\n      <th>mean_return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>greedy_memory</td>\n      <td>1.00</td>\n      <td>18.00</td>\n      <td>0.6600</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>greedy</td>\n      <td>0.36</td>\n      <td>169.18</td>\n      <td>-3.0164</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>random</td>\n      <td>0.16</td>\n      <td>234.00</td>\n      <td>-4.5168</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":39},{"id":"6cc81c9e-a247-4123-a3b2-11016cb655b0","cell_type":"markdown","source":"### Ergebnis der Baseline-Evaluation\n- Die Tabelle vergleicht Baselines über success_rate, mean_steps und return_mean.\n- greedy_memory erreicht das Ziel oft zuverlässig und braucht meist weniger Schritte, daher ist der Return oft höher.\n- greedy ist zielgerichtet, kann aber in Schleifen oder Sackgassen geraten, dadurch steigen Schritte und der Return sinkt.\n- random hat keine Zielstrategie und dient als Referenz für schlechtes, aber erwartbares Verhalten.","metadata":{}},{"id":"05135d4d-1d29-4f1a-b6f5-7f532cf676ee","cell_type":"markdown","source":"### Greedy mit Zufall und optional Memory\n- Greedy wählt meist den Schritt, der die Distanz zum Ziel reduziert.\n- p_random mischt Zufall ein und hilft, aus schlechten Routinen oder Sackgassen auszubrechen.\n- Memory vermeidet nach Möglichkeit bereits besuchte Felder und reduziert Wiederholungen.\n- Zusammen ergibt das eine gut spielbare Baseline ohne Lernen.\n- Die Wirkung von Zufall, Hindernisdichte und Schrittstrafe wird direkt sichtbar.","metadata":{}},{"id":"aef15e81-1f1e-4c4b-acfd-448bbfd15187","cell_type":"code","source":"# Interaktive Baseline-Demo: Greedy mit Zufall und Memory gegen Festlaufen\n# Spielbar: Parameter ändern, Episode laufen lassen, Verhalten direkt beobachten.\n\ntry:\n    import ipywidgets as w\nexcept Exception as e:\n    raise RuntimeError(\n        \"ipywidgets ist nicht verfügbar. Installiere es mit pip install ipywidgets. Fehler: %s\" % e\n    )\n\nimport random\nimport time\nfrom IPython.display import display\n\n\ndef manhattan(a, b):\n    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n\ndef generate_world_with_path(width, height, density=0.18, seed=0):\n    \"\"\"\n    Erzeugt eine Welt mit garantierter Verbindung zwischen Start und Ziel.\n    Erst wird ein einfacher Pfad gebaut, danach werden Wände zufällig gesetzt,\n    aber nicht auf dem Pfad.\n    \"\"\"\n    rng = random.Random(seed)\n    start = (0, 0)\n    goal = (width - 1, height - 1)\n\n    # Pfad: rechts/runter mit etwas Variation\n    path = {start}\n    x, y = start\n    while (x, y) != goal:\n        moves = []\n        if x < goal[0]:\n            moves.append((x + 1, y))\n        if y < goal[1]:\n            moves.append((x, y + 1))\n        nxt = rng.choice(moves)\n        x, y = nxt\n        path.add((x, y))\n\n    walls = set()\n    for yy in range(height):\n        for xx in range(width):\n            p = (xx, yy)\n            if p in path:\n                continue\n            if p == start or p == goal:\n                continue\n            if rng.random() < density:\n                walls.add(p)\n\n    return start, goal, walls\n\n\ndef render_grid(pos, goal, width, height, walls=None, trail=None):\n    walls = walls or set()\n    trail = trail or set()\n    px, py = pos\n    gx, gy = goal\n\n    lines = []\n    for y in range(height):\n        row = []\n        for x in range(width):\n            cell = \".\"\n            if (x, y) in walls:\n                cell = \"#\"\n            if (x, y) in trail:\n                cell = \"*\"\n            if (x, y) == (gx, gy):\n                cell = \"G\"\n            if (x, y) == (px, py):\n                cell = \"M\"\n            row.append(cell)\n        lines.append(\" \".join(row))\n    return \"\\n\".join(lines)\n\n\ndef _next_pos(pos, action, width, height, walls):\n    moves = {0: (0, -1), 1: (1, 0), 2: (0, 1), 3: (-1, 0)}  # up, right, down, left\n    dx, dy = moves[int(action)]\n    nxt = (pos[0] + dx, pos[1] + dy)\n    if not (0 <= nxt[0] < width and 0 <= nxt[1] < height):\n        return pos\n    if nxt in walls:\n        return pos\n    return nxt\n\n\ndef _valid_actions(pos, width, height, walls):\n    acts = []\n    for a in [0, 1, 2, 3]:\n        if _next_pos(pos, a, width, height, walls) != pos:\n            acts.append(a)\n    return acts\n\n\ndef _greedy_action_memory(pos, goal, width, height, walls, visited, explore_if_stuck=True):\n    \"\"\"\n    Greedy Richtung Ziel, aber bevorzugt unbesuchte Felder.\n    Wenn alle besten Schritte in besuchte Felder führen, kann er optional explorieren.\n    \"\"\"\n    best_d = 10**9\n    best_actions = []\n\n    for a in [0, 1, 2, 3]:\n        sp = _next_pos(pos, a, width, height, walls)\n        d = manhattan(sp, goal)\n        if d < best_d:\n            best_d = d\n            best_actions = [a]\n        elif d == best_d:\n            best_actions.append(a)\n\n    # unter den besten: unbesuchte bevorzugen\n    unvisited = [a for a in best_actions if _next_pos(pos, a, width, height, walls) not in visited]\n    if unvisited:\n        return random.choice(unvisited)\n\n    # wenn festgefahren: explorieren mit einem gültigen Schritt\n    if explore_if_stuck:\n        va = _valid_actions(pos, width, height, walls)\n        if va:\n            return random.choice(va)\n\n    return random.choice(best_actions) if best_actions else 0\n\n\ndef run_grid_visual(\n    out,\n    width=20,\n    height=10,\n    density=0.18,\n    seed=0,\n    p_random=0.20,\n    penalty=-0.02,\n    goal_reward=1.0,\n    delay=0.04,\n    show_trail=True,\n    max_steps=400,\n    use_memory=True,\n):\n    start, goal, walls = generate_world_with_path(width, height, density=density, seed=seed)\n    pos = start\n    total = 0.0\n    trail = set([pos])\n    visited = set([pos])\n\n    with out:\n        out.clear_output(wait=True)\n\n        for t in range(max_steps):\n            if show_trail:\n                trail.add(pos)\n\n            out.clear_output(wait=True)\n            print(\n                f\"t={t:03d}  pos={pos}  goal={goal}  return={total:.2f}  \"\n                f\"walls={len(walls)}  p_random={float(p_random):.2f}  memory={use_memory}\"\n            )\n            print(\n                render_grid(\n                    pos=pos,\n                    goal=goal,\n                    width=width,\n                    height=height,\n                    walls=walls,\n                    trail=trail if show_trail else None,\n                )\n            )\n\n            # Aktion wählen: entweder zufällig (Exploration), oder greedy (mit oder ohne Memory)\n            if random.random() < float(p_random):\n                va = _valid_actions(pos, width, height, walls)\n                action = random.choice(va) if va else random.choice([0, 1, 2, 3])\n            else:\n                if use_memory:\n                    action = _greedy_action_memory(pos, goal, width, height, walls, visited)\n                else:\n                    # reiner greedy fallback (ohne Memory)\n                    best_a = 0\n                    best_d = 10**9\n                    for a in [0, 1, 2, 3]:\n                        sp = _next_pos(pos, a, width, height, walls)\n                        d = manhattan(sp, goal)\n                        if d < best_d:\n                            best_d = d\n                            best_a = a\n                    action = best_a\n\n            pos = _next_pos(pos, action, width, height, walls)\n            visited.add(pos)\n\n            # Reward\n            if pos == goal:\n                total += float(goal_reward)\n                out.clear_output(wait=True)\n                print(\n                    f\"steps={t+1}  return={total:.2f}  reached_goal=True  walls={len(walls)}  memory={use_memory}\"\n                )\n                print(\n                    render_grid(\n                        pos=pos,\n                        goal=goal,\n                        width=width,\n                        height=height,\n                        walls=walls,\n                        trail=trail if show_trail else None,\n                    )\n                )\n                break\n            else:\n                total += float(penalty)\n\n            # Abbruch, wenn max_steps erreicht\n            if (t + 1) >= max_steps:\n                out.clear_output(wait=True)\n                print(\n                    f\"steps={t+1}  return={total:.2f}  reached_goal={pos==goal}  walls={len(walls)}  memory={use_memory}\"\n                )\n                print(\n                    render_grid(\n                        pos=pos,\n                        goal=goal,\n                        width=width,\n                        height=height,\n                        walls=walls,\n                        trail=trail if show_trail else None,\n                    )\n                )\n                break\n\n            time.sleep(float(delay))\n\n\n# UI\nbtn = w.Button(description=\"Run episode\", button_style=\"primary\")\nW = w.IntSlider(value=20, min=10, max=40, step=1, description=\"width\")\nH = w.IntSlider(value=10, min=5, max=20, step=1, description=\"height\")\nD = w.FloatSlider(value=0.18, min=0.00, max=0.40, step=0.02, description=\"density\")\nS = w.IntSlider(value=0, min=0, max=99, step=1, description=\"seed\")\n\nPR = w.FloatSlider(value=0.20, min=0.00, max=1.00, step=0.05, description=\"p_random\")\nPE = w.FloatSlider(value=-0.02, min=-0.30, max=0.00, step=0.01, description=\"penalty\")\nGR = w.FloatSlider(value=1.0, min=0.2, max=2.0, step=0.1, description=\"goal_reward\")\n\nDL = w.FloatSlider(value=0.04, min=0.01, max=0.20, step=0.01, description=\"delay\")\nTR = w.Checkbox(value=True, description=\"trail\")\nMEM = w.Checkbox(value=True, description=\"memory\")\n\nout = w.Output()\n\ndisplay(\n    w.VBox(\n        [\n            w.HBox([btn]),\n            w.HBox([W, H, D, S]),\n            w.HBox([PR, PE, GR, DL, TR, MEM]),\n            out,\n        ]\n    )\n)\n\n\ndef _on_click(_):\n    run_grid_visual(\n        out,\n        width=int(W.value),\n        height=int(H.value),\n        density=float(D.value),\n        seed=int(S.value),\n        p_random=float(PR.value),\n        penalty=float(PE.value),\n        goal_reward=float(GR.value),\n        delay=float(DL.value),\n        show_trail=bool(TR.value),\n        max_steps=400,\n        use_memory=bool(MEM.value),\n    )\n\n\nbtn.on_click(_on_click)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T11:16:46.937748Z","iopub.execute_input":"2026-02-13T11:16:46.938101Z","iopub.status.idle":"2026-02-13T11:16:47.005622Z","shell.execute_reply.started":"2026-02-13T11:16:46.938076Z","shell.execute_reply":"2026-02-13T11:16:47.004742Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='primary', description='Run episode', style=ButtonStyle()),)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c7fe181eaaa471abd2edf0802ae6ed4"}},"metadata":{}}],"execution_count":40},{"id":"aed715f5-408d-4219-b987-0fe3d2693204","cell_type":"markdown","source":"Interpretation:\n- Die Baseline folgt einer festen Entscheidungsregel und kann optional durch Zufall ergänzt werden, sie passt ihr Verhalten nicht durch Lernen an.\n- Effizienz zeigt sich vor allem in der Schrittzahl, da zusätzliche Schritte bei Schrittstrafen den Return typischerweise senken.\n- Zufall erhöht die Chance, aus ungünstigen Situationen herauszukommen, kann aber gleichzeitig die Strecke verlängern und die Ergebnisse stärker streuen.\n- Memory reduziert Wiederholungen, indem bereits besuchte Zustände nach Möglichkeit gemieden werden, was Schleifen seltener macht.","metadata":{}},{"id":"c0a9d827-6786-4172-b56e-7b3f90208f4b","cell_type":"markdown","source":"Danach übertragen wir dieselben Begriffe auf die 2D MiniGridworld und auf Q Learning.\n","metadata":{}},{"id":"95c9ecdb-0689-4d1e-a16e-8d4ac411d8da","cell_type":"markdown","source":"## Markov Eigenschaft und MDP\n- **Markov Eigenschaft:** Die Zukunft hängt nur vom aktuellen Zustand und der gewählten Aktion ab, nicht von der gesamten Vergangenheit.\n- **MDP Bestandteile:** Zustände, Aktionen, Übergänge, Rewards, Diskontfaktor.\n- **Wichtiger Punkt:** Wenn relevante Information aus der Vergangenheit fehlt, ist der Zustand unvollständig und das Problem ist praktisch nicht markovisch.\n- **Lösung:** Zustand erweitern, bis alle entscheidungsrelevanten Informationen enthalten sind, zum Beispiel durch ein Memory-Flag.\n","metadata":{}},{"id":"38317beb-16ab-4ef3-aef6-9fce09799746","cell_type":"code","source":"# Markov Demo mit Zustands Erweiterung durch ein Schaltermerkmal\nclass SwitchWorld:\n    ACTIONS = {0: (0, -1), 1: (1, 0), 2: (0, 1), 3: (-1, 0)}\n\n    def __init__(self, width=6, height=4, start=(0, 0), goal=(5, 3), switch=(2, 1)):\n        self.width, self.height = width, height\n        self.start, self.goal, self.switch = start, goal, switch\n        self.reset()\n\n    def reset(self):\n        self.pos = self.start\n        self.switch_on = False\n        return (self.pos, self.switch_on)\n\n    def step(self, action):\n        dx, dy = self.ACTIONS[int(action)]\n        x, y = self.pos\n        nx = max(0, min(self.width - 1, x + dx))\n        ny = max(0, min(self.height - 1, y + dy))\n        self.pos = (nx, ny)\n\n        if self.pos == self.switch:\n            self.switch_on = True\n\n        done = self.pos == self.goal\n        if done:\n            reward = 1.0 if self.switch_on else -1.0\n        else:\n            reward = -0.01\n\n        return (self.pos, self.switch_on), reward, done\n\n\ndef run_path(actions):\n    env = SwitchWorld()\n    obs = env.reset()\n    total = 0.0\n    for a in actions:\n        obs, r, done = env.step(a)\n        total += r\n        if done:\n            break\n    return obs, total\n\n\nobs_a, ret_a = run_path([1, 1, 1, 1, 1, 2, 2, 2])\nobs_b, ret_b = run_path([1, 1, 2, 1, 1, 1, 2, 2])\nprint('Ende A:', obs_a, 'Return:', round(ret_a, 3))\nprint('Ende B:', obs_b, 'Return:', round(ret_b, 3))\nprint('Bei gleicher Position kann der Reward verschieden sein, wenn Gedächtnis fehlt.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:37:40.336107Z","iopub.execute_input":"2026-02-13T10:37:40.336468Z","iopub.status.idle":"2026-02-13T10:37:40.348547Z","shell.execute_reply.started":"2026-02-13T10:37:40.336444Z","shell.execute_reply":"2026-02-13T10:37:40.347688Z"}},"outputs":[{"name":"stdout","text":"Ende A: ((5, 3), False) Return: -1.07\nEnde B: ((5, 3), True) Return: 0.93\nBei gleicher Position kann der Reward verschieden sein, wenn Gedächtnis fehlt.\n","output_type":"stream"}],"execution_count":29},{"id":"481d5cc0-b9fc-42e4-a46d-f4dafc7e41d4","cell_type":"markdown","source":"### Interpretation der Ausgabe (SwitchWorld)\n- **Gleiche Position, unterschiedliche Bewertung:** Ein Endzustand kann an derselben Position liegen, aber unterschiedliche Returns haben.\n- **Grund:** Der Zustand umfasst mehr als Koordinaten, zusätzlich zählt Kontext oder Gedächtnis.\n- **Konsequenz:** Ein reiner Positionszustand kann falsche Schlüsse erzwingen, weil die Aufgabe implizit Historie benötigt.\n- **Merksatz:** Markov wird eine Aufgabe erst, wenn der Zustand alle relevanten Informationen enthält.","metadata":{}},{"id":"df862863-155f-4c4a-b67d-c755d6b2dfc7","cell_type":"markdown","source":"## Q Learning mit Epsilon Decay","metadata":{}},{"id":"d8c3c5a0-5384-4065-a46c-f41c59e31ddc","cell_type":"markdown","source":"### Q Learning Update Regel\n\nQ Learning speichert für jeden Zustand und jede mögliche Aktion einen Q Wert.\nDer Q Wert steht für die erwartete Qualität einer Aktion, also wie gut sie sich langfristig auszahlt.\n\nDie zentrale Aktualisierung lautet:\n\nQ(s,a) = Q(s,a) + alpha * (r + gamma * max_a' Q(s',a') - Q(s,a))\n\nSo kann man die Formel lesen:\n- Der Ausdruck in Klammern ist das Lernsignal. Er vergleicht eine neue Schätzung mit dem bisherigen Q Wert.\n- r ist der sofortige Reward nach der Aktion.\n- max_a' Q(s',a') ist die beste erwartete Fortsetzung aus dem Folgezustand s'.\n- Der Term (r + gamma * max_a' Q(s',a')) ist das Ziel, auf das Q(s,a) hin bewegt wird.\n- Der Teil (r + gamma * max_a' Q(s',a') - Q(s,a)) ist der Fehler der aktuellen Schätzung, auch TD Error genannt.\n\nBegriffe:\n- alpha: Lernrate. Hoch bedeutet schnelle Anpassung, niedrig bedeutet langsamer, aber oft stabiler.\n- gamma: Weitblick. Hoch bedeutet, zukünftige Rewards zählen stark. Niedrig bedeutet, kurzfristige Rewards dominieren.\n- eps: Exploration. Mit einer Wahrscheinlichkeit eps probiert der Agent zufällige Aktionen aus, statt immer die aktuell beste zu wählen.","metadata":{}},{"id":"4e03152a-6f3e-4f1f-bc61-f83bd05b389a","cell_type":"code","source":"# Mini Demo: eine einzelne Q Aktualisierung\nq_sa = 0.10\nr = -0.02\ngamma = 0.95\nmax_q_next = 0.40\nalpha = 0.50\n\ntd_target = r + gamma * max_q_next\ntd_error = td_target - q_sa\nq_new = q_sa + alpha * td_error\n\nprint('Q alt:', round(q_sa, 4))\nprint('TD Target:', round(td_target, 4))\nprint('TD Error:', round(td_error, 4))\nprint('Q neu:', round(q_new, 4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:41:00.947340Z","iopub.execute_input":"2026-02-13T10:41:00.947705Z","iopub.status.idle":"2026-02-13T10:41:00.954746Z","shell.execute_reply.started":"2026-02-13T10:41:00.947680Z","shell.execute_reply":"2026-02-13T10:41:00.953765Z"}},"outputs":[{"name":"stdout","text":"Q alt: 0.1\nTD Target: 0.36\nTD Error: 0.26\nQ neu: 0.23\n","output_type":"stream"}],"execution_count":30},{"id":"4631e6a9-4c60-4a0c-99d7-648f02518a5d","cell_type":"markdown","source":"\n### Interpretation des Q Updates\n- **Q_old:** bisherige Schätzung für die Qualität einer Aktion in einem Zustand.\n- **td_target:** neue Zielschätzung aus aktuellem Reward und bester erwarteter Fortsetzung.\n- **td_error:** Differenz zwischen neuer Zielschätzung und alter Schätzung.\n- **Update:** Q wird um einen Anteil von td_error verschoben, gesteuert durch alpha.\n- **Intuition:** Große Fehler führen zu größeren Korrekturen, kleine Fehler zu kleinen Korrekturen.","metadata":{}},{"id":"59112c75-d596-445f-8d10-dc5c91db60ed","cell_type":"code","source":"# Formel in Aktion: mehrere Q Updates als kurzer Prozess\nalpha = 0.50\ngamma = 0.95\n\n# Beispielzustand mit vier Aktionswerten\nQ_state = np.array([0.10, 0.05, -0.02, 0.00], dtype=float)\n\n# Feste Updates: (step, s, a, r, maxQ_next)\nupdates = [\n    (1, 0, 0, -0.02, 0.40),\n    (2, 0, 0, -0.02, 0.35),\n    (3, 0, 0,  0.10, 0.30),\n    (4, 0, 0, -0.02, 0.45),\n    (5, 0, 0,  0.20, 0.50),\n]\n\nrows = []\nfor step, s, a, r, max_q_next in updates:\n    q_old = float(Q_state[a])\n    td_target = float(r + gamma * max_q_next)\n    td_error = float(td_target - q_old)\n    q_new = float(q_old + alpha * td_error)\n    Q_state[a] = q_new\n\n    rows.append({\n        'step': step,\n        'Q_old': round(q_old, 4),\n        'td_target': round(td_target, 4),\n        'td_error': round(td_error, 4),\n        'Q_new': round(q_new, 4),\n    })\n\ndf_updates = pd.DataFrame(rows, columns=['step', 'Q_old', 'td_target', 'td_error', 'Q_new'])\nprint(df_updates.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:42:03.504485Z","iopub.execute_input":"2026-02-13T10:42:03.505575Z","iopub.status.idle":"2026-02-13T10:42:03.517896Z","shell.execute_reply.started":"2026-02-13T10:42:03.505537Z","shell.execute_reply":"2026-02-13T10:42:03.516919Z"}},"outputs":[{"name":"stdout","text":" step  Q_old  td_target  td_error  Q_new\n    1 0.1000     0.3600    0.2600 0.2300\n    2 0.2300     0.3125    0.0825 0.2712\n    3 0.2712     0.3850    0.1138 0.3281\n    4 0.3281     0.4075    0.0794 0.3678\n    5 0.3678     0.6750    0.3072 0.5214\n","output_type":"stream"}],"execution_count":31},{"id":"ebf04a14-4e8f-445b-b711-d8fe966d7f5a","cell_type":"markdown","source":"### Interpretation der Update-Tabelle\n- **Wiederholtes Update:** Derselbe Q Wert nähert sich Schritt für Schritt der jeweiligen Zielschätzung an.\n- **Vorzeichen:** Wenn td_target größer ist als Q_old, steigt Q. Wenn td_target kleiner ist, sinkt Q.\n- **Dynamik:** Sprünge im td_target erzeugen größere Updates, weil neue Erfahrung die Erwartung verändert.\n- **Merksatz:** Lernen verläuft nicht glatt, sondern reagiert auf neue Rewards und neue Fortsetzungen.","metadata":{}},{"id":"58fafe69-74a8-4041-bf80-2f0c587b44a7","cell_type":"markdown","source":"### Policy aus der Q Tabelle\n- **Q Tabelle:** enthält pro Zustand und Aktion eine gelernte Qualitätsabschätzung.\n- **Policy:** Entscheidungsvorschrift, welche Aktion im Zustand gewählt wird.\n- **Hier:** Auswahl der Aktion mit dem höchsten Q Wert, also maximale erwartete Qualität.","metadata":{}},{"id":"9ee23969-154a-4de0-a1eb-6f3d8d980f69","cell_type":"code","source":"def policy_from_Q(Q):\n    return np.argmax(Q, axis=1)\n\n\ndef evaluate_with_returns(env, policy, episodes=50):\n    rets = []\n    steps = []\n    succ = 0\n\n    for _ in range(episodes):\n        s = env.reset()\n        sid = env.state_id(s)\n        total = 0.0\n\n        for t in range(env.max_steps):\n            a = int(policy[sid])\n            s, r, done, info = env.step(a)\n            total += r\n            sid = env.state_id(s)\n            if done:\n                succ += int(s == env.goal)\n                steps.append(info.get('t', t + 1))\n                break\n        else:\n            steps.append(env.max_steps)\n\n        rets.append(total)\n\n    return np.array(rets), np.array(steps), succ / episodes\n\n\ndef q_learning_train_decay(env, episodes=900, alpha=0.5, gamma=0.95,\n                           eps_start=0.35, eps_end=0.05):\n    n_states = env.width * env.height\n    Q = np.zeros((n_states, 4), dtype=float)\n    returns = []\n\n    for ep in range(episodes):\n        eps = eps_start + (eps_end - eps_start) * (ep / max(1, episodes - 1))\n        s = env.reset()\n        sid = env.state_id(s)\n        total = 0.0\n\n        for _ in range(env.max_steps):\n            if np.random.rand() < eps:\n                a = np.random.randint(0, 4)\n            else:\n                a = int(np.argmax(Q[sid]))\n\n            sp, r, done, _ = env.step(a)\n            spid = env.state_id(sp)\n            td_target = r + gamma * np.max(Q[spid])\n            Q[sid, a] += alpha * (td_target - Q[sid, a])\n\n            total += r\n            sid = spid\n            if done:\n                break\n\n        returns.append(total)\n\n    return Q, returns\n\n\ndef loss_function(success_rate, mean_steps, max_steps, returns, w1=1.0, w2=0.3, w3=0.1):\n    # Kombiniert Erfolgsrate, Effizienz und Varianz zu einem kompakten Vergleichswert\n    return (\n        w1 * (1.0 - float(success_rate)) +\n        w2 * (float(mean_steps) / float(max_steps)) +\n        w3 * float(np.var(returns))\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:43:32.572175Z","iopub.execute_input":"2026-02-13T10:43:32.573155Z","iopub.status.idle":"2026-02-13T10:43:32.585197Z","shell.execute_reply.started":"2026-02-13T10:43:32.573121Z","shell.execute_reply":"2026-02-13T10:43:32.584241Z"}},"outputs":[],"execution_count":32},{"id":"3f1d4a71-3a68-45d2-a28e-772fbb3c0273","cell_type":"markdown","source":"## Interaktiv trainieren und still ausführen\n- **Ziel:** Q Learning mit veränderbaren Parametern trainieren und das Ergebnis direkt prüfen.\n- **Train:** lernt eine Policy aus vielen Episoden und zeigt eine kompakte Auswertung.\n- **Auswertung:** Erfolg, Schrittzahl und Return als schnelle Qualitätsindikatoren.\n- **Stiller Run:** testet die gelernte Policy in genau einer Episode.\n- **Optional:** finaler ASCII Render zeigt den gefundenen Weg und mögliche Umwege.","metadata":{}},{"id":"2168ee4b-1924-4228-b8fc-cf5befb61576","cell_type":"markdown","source":"## Wie die Parameter das Lernen beeinflussen\n\n- alpha: Lernrate. Hoch bedeutet schnelle Anpassung der Q Werte, kann aber schwanken. Niedrig bedeutet langsameres, dafür oft stabileres Lernen.\n- gamma: Weitblick. Hoch bedeutet, dass zukünftige Belohnungen stark zählen. Niedrig bedeutet, dass kurzfristige Rewards dominieren.\n- eps_start: Startwert für Exploration. Hoch bedeutet mehr zufälliges Ausprobieren. Niedrig bedeutet mehr Exploitation, also schnelleres Nutzen der aktuell besten Aktion.\n- Nach einer Loss Suche werden Werte nur in die Regler übernommen. Drücke danach erneut Train, damit mit diesen Parametern wirklich neu gelernt wird.","metadata":{}},{"id":"bf605347-a082-4bc8-b39a-57cb549746f1","cell_type":"code","source":"try:\n    import ipywidgets as w\nexcept Exception as e:\n    raise RuntimeError('ipywidgets ist nicht verfügbar. Installiere es mit pip install ipywidgets. Fehler: %s' % e)\n\ntrain_btn = w.Button(description='Train', button_style='primary')\nquick_btn = w.Button(description='Quick Compare', button_style='info')\n\nW = w.IntSlider(value=20, min=10, max=40, step=1, description='width')\nH = w.IntSlider(value=10, min=5, max=30, step=1, description='height')\nD = w.FloatSlider(value=0.20, min=0.00, max=0.45, step=0.02, description='density')\nS = w.IntSlider(value=3, min=0, max=99, step=1, description='seed')\nSP = w.FloatSlider(value=-0.02, min=-0.30, max=0.00, step=0.01, description='step_penalty')\nGR = w.FloatSlider(value=1.0, min=0.2, max=2.0, step=0.1, description='goal_reward')\nMS = w.IntSlider(value=250, min=80, max=900, step=20, description='max_steps')\n\nEP = w.IntSlider(value=600, min=200, max=1000, step=100, description='episodes')\nAL = w.FloatSlider(value=0.50, min=0.05, max=1.0, step=0.05, description='alpha')\nGA = w.FloatSlider(value=0.95, min=0.10, max=0.99, step=0.05, description='gamma')\nEI = w.FloatSlider(value=0.35, min=0.00, max=1.0, step=0.05, description='eps_start')\nEE = w.FloatSlider(value=0.05, min=0.00, max=0.20, step=0.01, description='eps_end')\nSHOW_TRAIL = w.Checkbox(value=False, description='trail')\n\nout_train = w.Output()\nout_compare = w.Output()\n\ndisplay(w.VBox([\n    w.HBox([train_btn, quick_btn]),\n    w.HBox([W, H, D, S]),\n    w.HBox([SP, GR, MS]),\n    w.HBox([EP, AL, GA, EI, EE, SHOW_TRAIL]),\n    out_train,\n    out_compare,\n]))\n\nlast_policy = None\nlast_env = None\nlast_train_returns = None\nlast_train_params = None\n\nEVAL_EPISODES_FIXED = 40\n\ndef _build_env_from_slider():\n    return make_env(\n        width=int(W.value),\n        height=int(H.value),\n        density=float(D.value),\n        seed=int(S.value),\n        step_penalty=float(SP.value),\n        goal_reward=float(GR.value),\n        max_steps=int(MS.value),\n    )\n\ndef _run_episode_silent(env, policy, show_trail=False):\n    s = env.reset()\n    sid = env.state_id(s)\n    total = 0.0\n    trail = {s}\n\n    for t in range(env.max_steps):\n        a = int(policy[sid])\n        s, r, done, info = env.step(a)\n        sid = env.state_id(s)\n        total += r\n        if show_trail:\n            trail.add(s)\n        if done:\n            summary = {\n                'steps': int(info.get('t', t + 1)),\n                'return': float(total),\n                'reached_goal': bool(s == env.goal),\n            }\n            ascii_final = env.render(trail=trail if show_trail else None)\n            return summary, ascii_final\n\n    summary = {'steps': int(env.max_steps), 'return': float(total), 'reached_goal': bool(s == env.goal)}\n    ascii_final = env.render(trail=trail if show_trail else None)\n    return summary, ascii_final\n\ndef _train_once(eps_start_value, gamma_value, episodes_value, eval_episodes_value):\n    env_train = _build_env_from_slider()\n    Q, train_returns = q_learning_train_decay(\n        env_train,\n        episodes=int(episodes_value),\n        alpha=float(AL.value),\n        gamma=float(gamma_value),\n        eps_start=float(eps_start_value),\n        eps_end=float(EE.value),\n    )\n    policy = policy_from_Q(Q)\n\n    env_eval = _build_env_from_slider()\n    rets, steps, succ = evaluate_with_returns(env_eval, policy, episodes=int(eval_episodes_value))\n    metrics = {\n        'success_rate': float(succ),\n        'mean_steps': float(np.mean(steps)),\n        'return_mean': float(np.mean(rets)),\n        'var_return': float(np.var(rets)),\n        'loss': float(loss_function(float(succ), float(np.mean(steps)), int(MS.value), rets)),\n    }\n    return policy, train_returns, metrics\n\ndef _tip(success_rate):\n    if success_rate < 0.3:\n        return 'Tipp: erhöhe eps_start oder episodes, oder reduziere density.'\n    if success_rate <= 0.7:\n        return 'Tipp: erhöhe episodes oder senke eps_start leicht, um stabiler zu werden.'\n    return 'Tipp: senke eps_start, um weniger zufällig zu laufen, und prüfe mean_steps.'\n\ndef on_train(_):\n    global last_policy, last_env, last_train_returns, last_train_params\n\n    episodes_train = int(EP.value)\n    episodes_eval = int(EVAL_EPISODES_FIXED)\n    policy, train_returns, m = _train_once(EI.value, GA.value, episodes_train, episodes_eval)\n\n    last_policy = policy\n    last_env = _build_env_from_slider()\n    last_train_returns = train_returns\n    last_train_params = {\n        'width': int(W.value), 'height': int(H.value), 'density': float(D.value), 'seed': int(S.value),\n        'step_penalty': float(SP.value), 'goal_reward': float(GR.value), 'max_steps': int(MS.value),\n    }\n\n    env_label = f\"{int(W.value)}x{int(H.value)}, d={float(D.value):.2f}, s={int(S.value)}\"\n    row = {\n        'success_rate': m['success_rate'],\n        'mean_steps': m['mean_steps'],\n        'return_mean': m['return_mean'],\n        'var_return': m['var_return'],\n        'alpha': float(AL.value),\n        'gamma': float(GA.value),\n        'eps_start': float(EI.value),\n        'episodes': episodes_train,\n        'env': env_label,\n    }\n\n    with out_train:\n        out_train.clear_output(wait=True)\n        df = pd.DataFrame([row], columns=[\n            'success_rate', 'mean_steps', 'return_mean', 'var_return',\n            'alpha', 'gamma', 'eps_start', 'episodes', 'env'\n        ])\n        display(df)\n        print(_tip(m['success_rate']))\n        plt.figure(figsize=(4.2, 2.2))\n        plt.plot(train_returns)\n        plt.xlabel('ep')\n        plt.ylabel('ret')\n        plt.tight_layout()\n        plt.show()\n\ndef on_quick_compare(_):\n    eps_values = [0.05, 0.25, 0.50]\n    gamma_values = [0.20, 0.60, 0.95]\n    episodes_train = min(int(EP.value), 700)\n    episodes_eval = min(int(EVAL_EPISODES_FIXED), 40)\n\n    rows = []\n    for e0 in eps_values:\n        _, _, m = _train_once(e0, GA.value, episodes_train, episodes_eval)\n        rows.append({\n            'mode': 'eps_start',\n            'value': e0,\n            'success_rate': m['success_rate'],\n            'mean_steps': m['mean_steps'],\n            'loss': m['loss'],\n        })\n\n    for g0 in gamma_values:\n        _, _, m = _train_once(EI.value, g0, episodes_train, episodes_eval)\n        rows.append({\n            'mode': 'gamma',\n            'value': g0,\n            'success_rate': m['success_rate'],\n            'mean_steps': m['mean_steps'],\n            'loss': m['loss'],\n        })\n\n    with out_compare:\n        out_compare.clear_output(wait=True)\n        display(pd.DataFrame(rows).sort_values('loss', ascending=True).reset_index(drop=True))\n\ntrain_btn.on_click(on_train)\nquick_btn.on_click(on_quick_compare)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:53:07.650145Z","iopub.execute_input":"2026-02-13T10:53:07.651058Z","iopub.status.idle":"2026-02-13T10:53:07.718424Z","shell.execute_reply.started":"2026-02-13T10:53:07.651025Z","shell.execute_reply":"2026-02-13T10:53:07.717446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='primary', description='Train', style=ButtonStyle()), Button…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9c8d7a533b41bcb660a12b22c3a5d4"}},"metadata":{}}],"execution_count":35},{"id":"69db7a36-3419-43ec-b593-2bd4d76d7f60","cell_type":"markdown","source":"### Interpretation des Outputs\n- Der Return steigt im Training und stabilisiert sich, das spricht für eine bessere Policy mit weniger unnötigen Schritten.\n- eps_start hat hier nur geringen Einfluss, weil die Aufgabe unter diesen Einstellungen bereits leicht lösbar ist.\n- gamma wirkt stärker, bei zu kleinem gamma sinkt die Leistung, weil zukünftige Belohnungen zu wenig zählen.\n- Der Loss fasst Erfolg, Effizienz und Varianz zusammen, kleiner ist besser.","metadata":{}},{"id":"d62f2b8d-d513-4eeb-8d19-e0f65735a2b5","cell_type":"markdown","source":"### Return kurz erklärt\n- Return ist die Summe aller Rewards innerhalb einer Episode.\n- Im Trainingsplot ist der Return pro Episode über die Zeit dargestellt.\n- Eine Schrittstrafe senkt den Return mit jedem zusätzlichen Schritt, auch bei Zielerreichung.\n- Weniger Schritte bedeutet meist höheren Return, weil weniger Strafen anfallen und das Ziel schneller erreicht wird.","metadata":{}},{"id":"811379c4-ddfe-467f-8b13-9406853be899","cell_type":"code","source":"try:\n    import ipywidgets as w\nexcept Exception as e:\n    raise RuntimeError('ipywidgets ist nicht verfügbar. Installiere es mit pip install ipywidgets. Fehler: %s' % e)\n\nrun_btn = w.Button(description='Run still', button_style='success')\nshow_render = w.Checkbox(value=False, description='show render')\nout_run = w.Output()\n\ndisplay(w.VBox([w.HBox([run_btn, show_render]), out_run]))\n\ndef _run_policy_once(env, policy, render_ascii=False):\n    s = env.reset()\n    sid = env.state_id(s)\n    total = 0.0\n    trail = {s}\n\n    for t in range(env.max_steps):\n        a = int(policy[sid])\n        s, r, done, info = env.step(a)\n        sid = env.state_id(s)\n        total += r\n        trail.add(s)\n        if done:\n            summary = {\n                'steps': int(info.get('t', t + 1)),\n                'return': float(total),\n                'reached_goal': bool(s == env.goal),\n            }\n            final_ascii = env.render(trail=trail) if render_ascii else None\n            return summary, final_ascii\n\n    summary = {'steps': int(env.max_steps), 'return': float(total), 'reached_goal': bool(s == env.goal)}\n    final_ascii = env.render(trail=trail) if render_ascii else None\n    return summary, final_ascii\n\ndef on_run(_):\n    with out_run:\n        out_run.clear_output(wait=True)\n\n        if 'last_policy' not in globals() or last_policy is None:\n            print('Bitte zuerst trainieren.')\n            return\n\n        if 'last_train_params' in globals() and last_train_params is not None:\n            env = make_env(**last_train_params)\n        else:\n            env = make_env()\n\n        summary, ascii_final = _run_policy_once(env, last_policy, render_ascii=bool(show_render.value))\n        print('steps:', summary['steps'])\n        print('return:', round(summary['return'], 3))\n        print('reached_goal:', summary['reached_goal'])\n        if ascii_final is not None:\n            print(ascii_final)\n\nrun_btn.on_click(on_run)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:47:57.792491Z","iopub.execute_input":"2026-02-13T10:47:57.793887Z","iopub.status.idle":"2026-02-13T10:47:57.818591Z","shell.execute_reply.started":"2026-02-13T10:47:57.793803Z","shell.execute_reply":"2026-02-13T10:47:57.817505Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='success', description='Run still', style=ButtonStyle()), Ch…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7336bf130644ad58bff6080eb05d999"}},"metadata":{}}],"execution_count":34},{"id":"c81de5d7-4e89-4ce3-899d-6c5b25298f4b","cell_type":"markdown","source":"### Wie der Agent das Ziel nun optimiert erreicht\n- Der Agent nutzt eine gelernte Policy, die pro Zustand eine bevorzugte Aktion vorgibt.\n- Im Training werden Handlungen mit hohem langfristigem Return verstärkt, ungünstige Wege werden schrittweise entwertet.\n- Q Werte bilden eine Orientierung im Zustandsraum, gute Routen haben höhere Q Werte als Sackgassen oder Umwege.\n- Bei Schrittstrafen begünstigt die Policy kürzere Wege, dadurch sinken Schritte und der Return steigt.\n- Im finalen Lauf dominiert Exploitation, der Agent folgt stabil einer konsistenten Route statt stark zu variieren.","metadata":{}},{"id":"bbff6446-08a9-48ba-83dd-092c9ecf90a5","cell_type":"markdown","source":"## Mini Übungen\n\n1. Vergleiche in der Baseline Tabelle greedy und greedy_memory bei hoher Dichte.\n2. Setze im Training eps_start auf 0.0 und auf 0.5 und vergleiche die Erfolgsrate.\n3. Reduziere gamma auf 0.2 und erhöhe gamma auf 0.95. Beschreibe den Unterschied.\n4. Erhöhe die Schrittstrafe von minus 0.02 auf minus 0.08 und beobachte den Return.\n5. Nutze Loss Tuning und prüfe, ob die übernommenen Werte die Metriken verbessern.","metadata":{}},{"id":"49ac76f0-6563-4a8e-9492-7ee56a380dbb","cell_type":"markdown","source":"## Mini Leitfaden: Q Learning live ausprobieren (7 bis 10 Minuten)\n\n### 1) Start: Welt und Baselines\n1. Führe die Zellen bis zur Ausgabe der Baseline Agenten aus.\n2. Merke dir grob: Wie oft erreicht greedy das Ziel, wie viele Schritte braucht es im Mittel.\n\n### 2) Q Learning einmal trainieren\n1. Gehe zur interaktiven Sektion und drücke Train.\n2. Lies die Tabelle: success rate, mean steps und return.\n3. Führe danach den stillen Run aus und prüfe: reached goal, steps und return.\n\n### 3) Exploration verstehen (eps start)\n1. Setze eps start sehr niedrig, zum Beispiel 0.05, und trainiere erneut.\n2. Setze eps start höher, zum Beispiel 0.50, und trainiere erneut.\n3. Vergleiche success rate und mean steps.\nMerksatz: Hohe Exploration bedeutet mehr Ausprobieren. Das kann helfen, kostet aber oft mehr Schritte.\n\n### 4) Weitblick verstehen (gamma)\n1. Setze gamma niedrig, zum Beispiel 0.20, trainiere, und beobachte das Verhalten.\n2. Setze gamma hoch, zum Beispiel 0.95, trainiere, und vergleiche.\nMerksatz: Hoher Weitblick bewertet zukünftige Rewards stärker. Niedriger Weitblick bevorzugt kurzfristige Effekte.\n\n### 5) Lernrate verstehen (alpha)\n1. Setze alpha hoch, zum Beispiel 0.90, und trainiere.\n2. Setze alpha niedriger, zum Beispiel 0.20, und trainiere.\nMerksatz: Hohe Lernrate passt schnell an, kann aber instabil werden. Niedrige Lernrate ist langsamer, dafür oft stabiler.\n\n### 6) Schwierigkeit der Welt steuern\n1. Erhöhe density schrittweise, zum Beispiel von 0.15 auf 0.30.\n2. Beobachte, wann greedy scheitert und wann Q Learning deutlich besser wird.\nMerksatz: Wenn die Welt schwerer wird, braucht Lernen mehr Episoden und Exploration wird wichtiger.\n\n### 7) Return richtig deuten\nWenn es eine Schrittstrafe gibt, kann der Return trotz Erfolg klein oder negativ sein.\nWeniger Schritte bedeutet meist besseren Return, weil weniger Strafen gesammelt werden.\n\nOptional:\nNutze Quick Compare, um denselben Versuch mit mehreren eps start Werten direkt zu vergleichen.\n","metadata":{}}]}